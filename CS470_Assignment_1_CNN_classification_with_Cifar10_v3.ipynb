{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470 Assignment #1: CNN classification with Cifar10_v3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmoon/CS470/blob/master/CS470_Assignment_1_CNN_classification_with_Cifar10_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqPMAzzNipD",
        "colab_type": "text"
      },
      "source": [
        "CS470 Assignment #1: CNN classification with Cifar10\n",
        "====\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1mgGV_uOIK",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Connect to your Google Drive\n",
        "\n",
        "It is required if you want to save checkpoints and load them later on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLth6ZfXuSGT",
        "colab_type": "code",
        "outputId": "6bf3675c-a5b0-4ace-d523-8c059238343d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwUwGf8qW1U",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtshANjqpy4",
        "colab_type": "code",
        "outputId": "c369457c-192f-4923-a84e-fcb105075712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJ-Q6sbq8c3",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Configure the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5jAy7Wq-E2",
        "colab_type": "code",
        "outputId": "840060fb-fa11-423a-b92a-f1ec40763743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 200\n",
        "learning_rate = 0.001\n",
        "batch_size = 20000\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://3d837a87.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tZt60aMrQ1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Construct data pipeline\n",
        "\n",
        "**`torchvision.datasets.CIFAR10`** will automatically construct **`Cifar10`** dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbtV46LrXOF",
        "colab_type": "code",
        "outputId": "134b6232-cfac-4b44-a794-facbd87e58fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_dir = os.path.join(gdrive_root, 'my_data')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomCrop(32, padding=4),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_dWd-6rwWb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Construct a neural network builder\n",
        "\n",
        "We serve the baseline CNN model which is supported on Pytorch tutorial: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb#scrollTo=c1E1b7-igUcR\n",
        "\n",
        "### (You have to compare your own CNN model's test accuracy with the baseline CNN model and explain why your own model's test accuracy is higher than the basline.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_wne0Vr1E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([out,x], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyDenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
        "        super(MyDenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        num_planes = 2*growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans2 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans3 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_planes, self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
        "\n",
        "def DenseNet169():\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
        "\n",
        "def DenseNet201():\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
        "\n",
        "def DenseNet161():\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
        "\n",
        "def densenet_cifar():\n",
        "    return MyDenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpA3xhjMspvA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Initialize the network and optimizer\n",
        "\n",
        "If you want to train modularized neural network in Step 5B, please use 'MyClassifier2' as 'my_classifier'. It is written as a comment now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP111gW0s8aH",
        "colab_type": "code",
        "outputId": "5767f531-2c15-4af7-c801-7369383fefdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = 'cuda'\n",
        "my_classifier = densenet_cifar()\n",
        "my_classifier = my_classifier.to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        " \n",
        "\n",
        "# Print your neural network structure\n",
        "# print(my_classifier)\n",
        "print('parameter size : ', count_parameters(my_classifier))\n",
        "\n",
        "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter size :  1000618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAQeXmjsILS",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Load pre-trained weights if exist\n",
        "\n",
        "- **For your sumbmission you have to store the trained model as a checkpoint.**\n",
        "- Please do not erase this step.\n",
        "- If you want to modify this step, please be careful.\n",
        "- After training please confirm that your checkpoint is correctly stored and re-loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLNZxaBsHUl",
        "colab_type": "code",
        "outputId": "b9d5bd29-364d-4404-a68e-022accf3e11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best accuracy : 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHGB0Bcz5aA",
        "colab_type": "code",
        "outputId": "fbcce27f-8889-4f40-e3cd-3a94f364d529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "  !nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 22 23:51:25 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    58W / 149W |    360MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1t7n6yttNEc",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train the network\n",
        "\n",
        "Note : It would be better to save checkpoints periodically, otherwise you'll lose everything you've trained if the session is recycled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vczdKbytV38",
        "colab_type": "code",
        "outputId": "3ab2b06a-6b90-4d1a-892a-b768689d8314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if training_process:\n",
        "  it = 0\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(max_epoch):\n",
        "    # train phase\n",
        "    my_classifier.train()\n",
        "    for inputs, labels in train_dataloader:\n",
        "      it += 1\n",
        "\n",
        "      # load data to the GPU.\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # feed data into the network and get outputs.\n",
        "      logits = my_classifier(inputs)\n",
        "\n",
        "      # calculate loss\n",
        "      # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
        "      loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "      # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "      #       Otherwise, gradients will accumulate.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backprogate loss.\n",
        "      loss.backward()\n",
        "\n",
        "      # update the weights in the network.\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate accuracy.\n",
        "      acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "      if it % 200 == 0:\n",
        "        tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
        "        tbc.save_value('Accuracy', 'train_acc', it, acc.item())\n",
        "        print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
        "\n",
        "    # save losses in a list so that we can visualize them later.\n",
        "    train_losses.append(loss)  \n",
        "\n",
        "    # test phase\n",
        "    n = 0.\n",
        "    test_loss = 0.\n",
        "    test_acc = 0.\n",
        "    my_classifier.eval()\n",
        "    for test_inputs, test_labels in test_dataloader:\n",
        "      test_inputs = test_inputs.to(device)\n",
        "      test_labels = test_labels.to(device)\n",
        "\n",
        "      logits = my_classifier(test_inputs)\n",
        "      test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "      test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "      n += test_inputs.size(0)\n",
        "\n",
        "    test_loss /= n\n",
        "    test_acc /= n\n",
        "    test_losses.append(test_loss)\n",
        "    tbc.save_value('Loss', 'test_loss', it, test_loss)\n",
        "    tbc.save_value('Accuracy', 'test_acc', it, test_acc)\n",
        "    print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss, test_acc)) \n",
        "\n",
        "    tbc.flush_line('train_loss')\n",
        "    tbc.flush_line('test_loss')\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'my_classifier':my_classifier.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_acc':best_acc}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    \n",
        "tbc.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:101: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "[epoch:0, iteration:200] train loss : 0.0607 train accuracy : 0.9700\n",
            "[epoch:0, iteration:250] test_loss : 0.3925 test accuracy : 0.9099\n",
            "checkpoint is saved !\n",
            "[epoch:1, iteration:400] train loss : 0.0371 train accuracy : 0.9900\n",
            "[epoch:1, iteration:500] test_loss : 0.4183 test accuracy : 0.9070\n",
            "[epoch:2, iteration:600] train loss : 0.0490 train accuracy : 0.9850\n",
            "[epoch:2, iteration:750] test_loss : 0.4534 test accuracy : 0.9077\n",
            "[epoch:3, iteration:800] train loss : 0.0077 train accuracy : 1.0000\n",
            "[epoch:3, iteration:1000] train loss : 0.0228 train accuracy : 0.9950\n",
            "[epoch:3, iteration:1000] test_loss : 0.4472 test accuracy : 0.9078\n",
            "[epoch:4, iteration:1200] train loss : 0.0371 train accuracy : 0.9900\n",
            "[epoch:4, iteration:1250] test_loss : 0.4648 test accuracy : 0.9053\n",
            "[epoch:5, iteration:1400] train loss : 0.0094 train accuracy : 1.0000\n",
            "[epoch:5, iteration:1500] test_loss : 0.4532 test accuracy : 0.9045\n",
            "[epoch:6, iteration:1600] train loss : 0.0234 train accuracy : 0.9950\n",
            "[epoch:6, iteration:1750] test_loss : 0.4679 test accuracy : 0.9055\n",
            "[epoch:7, iteration:1800] train loss : 0.0331 train accuracy : 0.9850\n",
            "[epoch:7, iteration:2000] train loss : 0.0138 train accuracy : 0.9900\n",
            "[epoch:7, iteration:2000] test_loss : 0.4789 test accuracy : 0.9072\n",
            "[epoch:8, iteration:2200] train loss : 0.0291 train accuracy : 0.9900\n",
            "[epoch:8, iteration:2250] test_loss : 0.4539 test accuracy : 0.9043\n",
            "[epoch:9, iteration:2400] train loss : 0.0133 train accuracy : 0.9900\n",
            "[epoch:9, iteration:2500] test_loss : 0.4488 test accuracy : 0.9103\n",
            "checkpoint is saved !\n",
            "[epoch:10, iteration:2600] train loss : 0.0184 train accuracy : 0.9950\n",
            "[epoch:10, iteration:2750] test_loss : 0.4613 test accuracy : 0.9078\n",
            "[epoch:11, iteration:2800] train loss : 0.0189 train accuracy : 0.9900\n",
            "[epoch:11, iteration:3000] train loss : 0.0441 train accuracy : 0.9850\n",
            "[epoch:11, iteration:3000] test_loss : 0.4721 test accuracy : 0.9068\n",
            "[epoch:12, iteration:3200] train loss : 0.0236 train accuracy : 0.9950\n",
            "[epoch:12, iteration:3250] test_loss : 0.5053 test accuracy : 0.9024\n",
            "[epoch:13, iteration:3400] train loss : 0.0119 train accuracy : 0.9950\n",
            "[epoch:13, iteration:3500] test_loss : 0.4772 test accuracy : 0.9044\n",
            "[epoch:14, iteration:3600] train loss : 0.0443 train accuracy : 0.9800\n",
            "[epoch:14, iteration:3750] test_loss : 0.4960 test accuracy : 0.9038\n",
            "[epoch:15, iteration:3800] train loss : 0.0392 train accuracy : 0.9900\n",
            "[epoch:15, iteration:4000] train loss : 0.0505 train accuracy : 0.9950\n",
            "[epoch:15, iteration:4000] test_loss : 0.4765 test accuracy : 0.9029\n",
            "[epoch:16, iteration:4200] train loss : 0.0311 train accuracy : 0.9900\n",
            "[epoch:16, iteration:4250] test_loss : 0.4760 test accuracy : 0.9083\n",
            "[epoch:17, iteration:4400] train loss : 0.0107 train accuracy : 0.9950\n",
            "[epoch:17, iteration:4500] test_loss : 0.4789 test accuracy : 0.9082\n",
            "[epoch:18, iteration:4600] train loss : 0.0226 train accuracy : 0.9900\n",
            "[epoch:18, iteration:4750] test_loss : 0.4668 test accuracy : 0.9070\n",
            "[epoch:19, iteration:4800] train loss : 0.0097 train accuracy : 1.0000\n",
            "[epoch:19, iteration:5000] train loss : 0.0106 train accuracy : 1.0000\n",
            "[epoch:19, iteration:5000] test_loss : 0.4840 test accuracy : 0.9046\n",
            "[epoch:20, iteration:5200] train loss : 0.0431 train accuracy : 0.9850\n",
            "[epoch:20, iteration:5250] test_loss : 0.4751 test accuracy : 0.9095\n",
            "[epoch:21, iteration:5400] train loss : 0.0196 train accuracy : 0.9950\n",
            "[epoch:21, iteration:5500] test_loss : 0.4922 test accuracy : 0.9054\n",
            "[epoch:22, iteration:5600] train loss : 0.0191 train accuracy : 0.9950\n",
            "[epoch:22, iteration:5750] test_loss : 0.4920 test accuracy : 0.9074\n",
            "[epoch:23, iteration:5800] train loss : 0.0584 train accuracy : 0.9850\n",
            "[epoch:23, iteration:6000] train loss : 0.0644 train accuracy : 0.9800\n",
            "[epoch:23, iteration:6000] test_loss : 0.4764 test accuracy : 0.9068\n",
            "[epoch:24, iteration:6200] train loss : 0.0183 train accuracy : 0.9950\n",
            "[epoch:24, iteration:6250] test_loss : 0.5012 test accuracy : 0.9057\n",
            "[epoch:25, iteration:6400] train loss : 0.0119 train accuracy : 0.9900\n",
            "[epoch:25, iteration:6500] test_loss : 0.5234 test accuracy : 0.9028\n",
            "[epoch:26, iteration:6600] train loss : 0.0297 train accuracy : 0.9850\n",
            "[epoch:26, iteration:6750] test_loss : 0.4873 test accuracy : 0.9070\n",
            "[epoch:27, iteration:6800] train loss : 0.0032 train accuracy : 1.0000\n",
            "[epoch:27, iteration:7000] train loss : 0.0135 train accuracy : 0.9950\n",
            "[epoch:27, iteration:7000] test_loss : 0.4845 test accuracy : 0.9075\n",
            "[epoch:28, iteration:7200] train loss : 0.0061 train accuracy : 1.0000\n",
            "[epoch:28, iteration:7250] test_loss : 0.5001 test accuracy : 0.9010\n",
            "[epoch:29, iteration:7400] train loss : 0.0436 train accuracy : 0.9850\n",
            "[epoch:29, iteration:7500] test_loss : 0.4869 test accuracy : 0.9060\n",
            "[epoch:30, iteration:7600] train loss : 0.0182 train accuracy : 0.9900\n",
            "[epoch:30, iteration:7750] test_loss : 0.4825 test accuracy : 0.9070\n",
            "[epoch:31, iteration:7800] train loss : 0.0319 train accuracy : 0.9950\n",
            "[epoch:31, iteration:8000] train loss : 0.0365 train accuracy : 0.9850\n",
            "[epoch:31, iteration:8000] test_loss : 0.4862 test accuracy : 0.9088\n",
            "[epoch:32, iteration:8200] train loss : 0.0228 train accuracy : 0.9900\n",
            "[epoch:32, iteration:8250] test_loss : 0.4869 test accuracy : 0.9077\n",
            "[epoch:33, iteration:8400] train loss : 0.0256 train accuracy : 0.9900\n",
            "[epoch:33, iteration:8500] test_loss : 0.4892 test accuracy : 0.9109\n",
            "checkpoint is saved !\n",
            "[epoch:34, iteration:8600] train loss : 0.0347 train accuracy : 0.9900\n",
            "[epoch:34, iteration:8750] test_loss : 0.4608 test accuracy : 0.9111\n",
            "checkpoint is saved !\n",
            "[epoch:35, iteration:8800] train loss : 0.0162 train accuracy : 0.9950\n",
            "[epoch:35, iteration:9000] train loss : 0.0275 train accuracy : 0.9950\n",
            "[epoch:35, iteration:9000] test_loss : 0.4814 test accuracy : 0.9083\n",
            "[epoch:36, iteration:9200] train loss : 0.0189 train accuracy : 0.9950\n",
            "[epoch:36, iteration:9250] test_loss : 0.4891 test accuracy : 0.9077\n",
            "[epoch:37, iteration:9400] train loss : 0.0192 train accuracy : 0.9900\n",
            "[epoch:37, iteration:9500] test_loss : 0.4876 test accuracy : 0.9080\n",
            "[epoch:38, iteration:9600] train loss : 0.0303 train accuracy : 0.9800\n",
            "[epoch:38, iteration:9750] test_loss : 0.4695 test accuracy : 0.9089\n",
            "[epoch:39, iteration:9800] train loss : 0.0301 train accuracy : 0.9850\n",
            "[epoch:39, iteration:10000] train loss : 0.0400 train accuracy : 0.9800\n",
            "[epoch:39, iteration:10000] test_loss : 0.4929 test accuracy : 0.9084\n",
            "[epoch:40, iteration:10200] train loss : 0.0107 train accuracy : 0.9950\n",
            "[epoch:40, iteration:10250] test_loss : 0.4792 test accuracy : 0.9120\n",
            "checkpoint is saved !\n",
            "[epoch:41, iteration:10400] train loss : 0.0148 train accuracy : 0.9950\n",
            "[epoch:41, iteration:10500] test_loss : 0.5028 test accuracy : 0.9054\n",
            "[epoch:42, iteration:10600] train loss : 0.0145 train accuracy : 0.9950\n",
            "[epoch:42, iteration:10750] test_loss : 0.4885 test accuracy : 0.9064\n",
            "[epoch:43, iteration:10800] train loss : 0.0040 train accuracy : 1.0000\n",
            "[epoch:43, iteration:11000] train loss : 0.0238 train accuracy : 0.9950\n",
            "[epoch:43, iteration:11000] test_loss : 0.5232 test accuracy : 0.9056\n",
            "[epoch:44, iteration:11200] train loss : 0.0418 train accuracy : 0.9850\n",
            "[epoch:44, iteration:11250] test_loss : 0.4921 test accuracy : 0.9102\n",
            "[epoch:45, iteration:11400] train loss : 0.0867 train accuracy : 0.9750\n",
            "[epoch:45, iteration:11500] test_loss : 0.4970 test accuracy : 0.9080\n",
            "[epoch:46, iteration:11600] train loss : 0.0172 train accuracy : 0.9950\n",
            "[epoch:46, iteration:11750] test_loss : 0.4877 test accuracy : 0.9112\n",
            "[epoch:47, iteration:11800] train loss : 0.0137 train accuracy : 0.9950\n",
            "[epoch:47, iteration:12000] train loss : 0.0112 train accuracy : 0.9950\n",
            "[epoch:47, iteration:12000] test_loss : 0.5074 test accuracy : 0.9068\n",
            "[epoch:48, iteration:12200] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:48, iteration:12250] test_loss : 0.4910 test accuracy : 0.9081\n",
            "[epoch:49, iteration:12400] train loss : 0.0120 train accuracy : 0.9950\n",
            "[epoch:49, iteration:12500] test_loss : 0.4898 test accuracy : 0.9082\n",
            "[epoch:50, iteration:12600] train loss : 0.0061 train accuracy : 0.9950\n",
            "[epoch:50, iteration:12750] test_loss : 0.4874 test accuracy : 0.9109\n",
            "[epoch:51, iteration:12800] train loss : 0.0089 train accuracy : 1.0000\n",
            "[epoch:51, iteration:13000] train loss : 0.0330 train accuracy : 0.9950\n",
            "[epoch:51, iteration:13000] test_loss : 0.4962 test accuracy : 0.9109\n",
            "[epoch:52, iteration:13200] train loss : 0.0145 train accuracy : 0.9950\n",
            "[epoch:52, iteration:13250] test_loss : 0.5316 test accuracy : 0.9103\n",
            "[epoch:53, iteration:13400] train loss : 0.0045 train accuracy : 1.0000\n",
            "[epoch:53, iteration:13500] test_loss : 0.4858 test accuracy : 0.9093\n",
            "[epoch:54, iteration:13600] train loss : 0.0461 train accuracy : 0.9850\n",
            "[epoch:54, iteration:13750] test_loss : 0.4872 test accuracy : 0.9094\n",
            "[epoch:55, iteration:13800] train loss : 0.0073 train accuracy : 0.9950\n",
            "[epoch:55, iteration:14000] train loss : 0.0253 train accuracy : 0.9900\n",
            "[epoch:55, iteration:14000] test_loss : 0.4920 test accuracy : 0.9129\n",
            "checkpoint is saved !\n",
            "[epoch:56, iteration:14200] train loss : 0.0166 train accuracy : 0.9900\n",
            "[epoch:56, iteration:14250] test_loss : 0.4816 test accuracy : 0.9101\n",
            "[epoch:57, iteration:14400] train loss : 0.0352 train accuracy : 0.9850\n",
            "[epoch:57, iteration:14500] test_loss : 0.5290 test accuracy : 0.9100\n",
            "[epoch:58, iteration:14600] train loss : 0.0075 train accuracy : 1.0000\n",
            "[epoch:58, iteration:14750] test_loss : 0.5122 test accuracy : 0.9063\n",
            "[epoch:59, iteration:14800] train loss : 0.0198 train accuracy : 0.9950\n",
            "[epoch:59, iteration:15000] train loss : 0.0408 train accuracy : 0.9850\n",
            "[epoch:59, iteration:15000] test_loss : 0.5291 test accuracy : 0.9086\n",
            "[epoch:60, iteration:15200] train loss : 0.0110 train accuracy : 0.9950\n",
            "[epoch:60, iteration:15250] test_loss : 0.5325 test accuracy : 0.9069\n",
            "[epoch:61, iteration:15400] train loss : 0.0190 train accuracy : 0.9900\n",
            "[epoch:61, iteration:15500] test_loss : 0.5241 test accuracy : 0.9085\n",
            "[epoch:62, iteration:15600] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:62, iteration:15750] test_loss : 0.5013 test accuracy : 0.9091\n",
            "[epoch:63, iteration:15800] train loss : 0.0102 train accuracy : 0.9950\n",
            "[epoch:63, iteration:16000] train loss : 0.0111 train accuracy : 0.9950\n",
            "[epoch:63, iteration:16000] test_loss : 0.5191 test accuracy : 0.9112\n",
            "[epoch:64, iteration:16200] train loss : 0.0168 train accuracy : 0.9950\n",
            "[epoch:64, iteration:16250] test_loss : 0.5180 test accuracy : 0.9101\n",
            "[epoch:65, iteration:16400] train loss : 0.0156 train accuracy : 0.9950\n",
            "[epoch:65, iteration:16500] test_loss : 0.5065 test accuracy : 0.9104\n",
            "[epoch:66, iteration:16600] train loss : 0.0276 train accuracy : 0.9900\n",
            "[epoch:66, iteration:16750] test_loss : 0.5046 test accuracy : 0.9095\n",
            "[epoch:67, iteration:16800] train loss : 0.0036 train accuracy : 1.0000\n",
            "[epoch:67, iteration:17000] train loss : 0.0079 train accuracy : 0.9950\n",
            "[epoch:67, iteration:17000] test_loss : 0.5414 test accuracy : 0.9078\n",
            "[epoch:68, iteration:17200] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:68, iteration:17250] test_loss : 0.5240 test accuracy : 0.9079\n",
            "[epoch:69, iteration:17400] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:69, iteration:17500] test_loss : 0.5069 test accuracy : 0.9075\n",
            "[epoch:70, iteration:17600] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:70, iteration:17750] test_loss : 0.5333 test accuracy : 0.9078\n",
            "[epoch:71, iteration:17800] train loss : 0.0471 train accuracy : 0.9800\n",
            "[epoch:71, iteration:18000] train loss : 0.0062 train accuracy : 0.9950\n",
            "[epoch:71, iteration:18000] test_loss : 0.5404 test accuracy : 0.9074\n",
            "[epoch:72, iteration:18200] train loss : 0.0101 train accuracy : 0.9950\n",
            "[epoch:72, iteration:18250] test_loss : 0.5363 test accuracy : 0.9074\n",
            "[epoch:73, iteration:18400] train loss : 0.0096 train accuracy : 1.0000\n",
            "[epoch:73, iteration:18500] test_loss : 0.5191 test accuracy : 0.9101\n",
            "[epoch:74, iteration:18600] train loss : 0.0022 train accuracy : 1.0000\n",
            "[epoch:74, iteration:18750] test_loss : 0.5519 test accuracy : 0.9069\n",
            "[epoch:75, iteration:18800] train loss : 0.0038 train accuracy : 1.0000\n",
            "[epoch:75, iteration:19000] train loss : 0.0037 train accuracy : 1.0000\n",
            "[epoch:75, iteration:19000] test_loss : 0.5150 test accuracy : 0.9091\n",
            "[epoch:76, iteration:19200] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:76, iteration:19250] test_loss : 0.5216 test accuracy : 0.9090\n",
            "[epoch:77, iteration:19400] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:77, iteration:19500] test_loss : 0.5226 test accuracy : 0.9155\n",
            "checkpoint is saved !\n",
            "[epoch:78, iteration:19600] train loss : 0.0135 train accuracy : 0.9950\n",
            "[epoch:78, iteration:19750] test_loss : 0.5372 test accuracy : 0.9088\n",
            "[epoch:79, iteration:19800] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:79, iteration:20000] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:79, iteration:20000] test_loss : 0.4998 test accuracy : 0.9129\n",
            "[epoch:80, iteration:20200] train loss : 0.0387 train accuracy : 0.9950\n",
            "[epoch:80, iteration:20250] test_loss : 0.5423 test accuracy : 0.9095\n",
            "[epoch:81, iteration:20400] train loss : 0.0028 train accuracy : 1.0000\n",
            "[epoch:81, iteration:20500] test_loss : 0.5348 test accuracy : 0.9085\n",
            "[epoch:82, iteration:20600] train loss : 0.0121 train accuracy : 0.9950\n",
            "[epoch:82, iteration:20750] test_loss : 0.5310 test accuracy : 0.9114\n",
            "[epoch:83, iteration:20800] train loss : 0.0106 train accuracy : 0.9950\n",
            "[epoch:83, iteration:21000] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:83, iteration:21000] test_loss : 0.4994 test accuracy : 0.9077\n",
            "[epoch:84, iteration:21200] train loss : 0.0057 train accuracy : 1.0000\n",
            "[epoch:84, iteration:21250] test_loss : 0.5382 test accuracy : 0.9092\n",
            "[epoch:85, iteration:21400] train loss : 0.0016 train accuracy : 1.0000\n",
            "[epoch:85, iteration:21500] test_loss : 0.5662 test accuracy : 0.9061\n",
            "[epoch:86, iteration:21600] train loss : 0.0095 train accuracy : 0.9950\n",
            "[epoch:86, iteration:21750] test_loss : 0.5265 test accuracy : 0.9114\n",
            "[epoch:87, iteration:21800] train loss : 0.0100 train accuracy : 1.0000\n",
            "[epoch:87, iteration:22000] train loss : 0.0071 train accuracy : 0.9950\n",
            "[epoch:87, iteration:22000] test_loss : 0.5086 test accuracy : 0.9135\n",
            "[epoch:88, iteration:22200] train loss : 0.0136 train accuracy : 0.9950\n",
            "[epoch:88, iteration:22250] test_loss : 0.5135 test accuracy : 0.9085\n",
            "[epoch:89, iteration:22400] train loss : 0.0049 train accuracy : 1.0000\n",
            "[epoch:89, iteration:22500] test_loss : 0.5042 test accuracy : 0.9121\n",
            "[epoch:90, iteration:22600] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:90, iteration:22750] test_loss : 0.5326 test accuracy : 0.9089\n",
            "[epoch:91, iteration:22800] train loss : 0.0129 train accuracy : 0.9950\n",
            "[epoch:91, iteration:23000] train loss : 0.0136 train accuracy : 0.9950\n",
            "[epoch:91, iteration:23000] test_loss : 0.5438 test accuracy : 0.9098\n",
            "[epoch:92, iteration:23200] train loss : 0.0077 train accuracy : 0.9950\n",
            "[epoch:92, iteration:23250] test_loss : 0.5246 test accuracy : 0.9093\n",
            "[epoch:93, iteration:23400] train loss : 0.0235 train accuracy : 0.9950\n",
            "[epoch:93, iteration:23500] test_loss : 0.5355 test accuracy : 0.9098\n",
            "[epoch:94, iteration:23600] train loss : 0.0139 train accuracy : 0.9950\n",
            "[epoch:94, iteration:23750] test_loss : 0.5482 test accuracy : 0.9097\n",
            "[epoch:95, iteration:23800] train loss : 0.0247 train accuracy : 0.9900\n",
            "[epoch:95, iteration:24000] train loss : 0.0102 train accuracy : 0.9950\n",
            "[epoch:95, iteration:24000] test_loss : 0.5286 test accuracy : 0.9099\n",
            "[epoch:96, iteration:24200] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:96, iteration:24250] test_loss : 0.5579 test accuracy : 0.9091\n",
            "[epoch:97, iteration:24400] train loss : 0.0069 train accuracy : 0.9950\n",
            "[epoch:97, iteration:24500] test_loss : 0.5273 test accuracy : 0.9088\n",
            "[epoch:98, iteration:24600] train loss : 0.0175 train accuracy : 0.9950\n",
            "[epoch:98, iteration:24750] test_loss : 0.5263 test accuracy : 0.9100\n",
            "[epoch:99, iteration:24800] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:99, iteration:25000] train loss : 0.0307 train accuracy : 0.9900\n",
            "[epoch:99, iteration:25000] test_loss : 0.5197 test accuracy : 0.9140\n",
            "[epoch:100, iteration:25200] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:100, iteration:25250] test_loss : 0.5406 test accuracy : 0.9132\n",
            "[epoch:101, iteration:25400] train loss : 0.0029 train accuracy : 1.0000\n",
            "[epoch:101, iteration:25500] test_loss : 0.5258 test accuracy : 0.9090\n",
            "[epoch:102, iteration:25600] train loss : 0.0092 train accuracy : 0.9950\n",
            "[epoch:102, iteration:25750] test_loss : 0.5220 test accuracy : 0.9107\n",
            "[epoch:103, iteration:25800] train loss : 0.0186 train accuracy : 0.9950\n",
            "[epoch:103, iteration:26000] train loss : 0.0327 train accuracy : 0.9850\n",
            "[epoch:103, iteration:26000] test_loss : 0.5275 test accuracy : 0.9098\n",
            "[epoch:104, iteration:26200] train loss : 0.0336 train accuracy : 0.9850\n",
            "[epoch:104, iteration:26250] test_loss : 0.5238 test accuracy : 0.9116\n",
            "[epoch:105, iteration:26400] train loss : 0.0056 train accuracy : 1.0000\n",
            "[epoch:105, iteration:26500] test_loss : 0.5336 test accuracy : 0.9056\n",
            "[epoch:106, iteration:26600] train loss : 0.0115 train accuracy : 1.0000\n",
            "[epoch:106, iteration:26750] test_loss : 0.5292 test accuracy : 0.9114\n",
            "[epoch:107, iteration:26800] train loss : 0.0209 train accuracy : 0.9900\n",
            "[epoch:107, iteration:27000] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:107, iteration:27000] test_loss : 0.5591 test accuracy : 0.9084\n",
            "[epoch:108, iteration:27200] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:108, iteration:27250] test_loss : 0.5626 test accuracy : 0.9063\n",
            "[epoch:109, iteration:27400] train loss : 0.0045 train accuracy : 1.0000\n",
            "[epoch:109, iteration:27500] test_loss : 0.5373 test accuracy : 0.9091\n",
            "[epoch:110, iteration:27600] train loss : 0.0095 train accuracy : 1.0000\n",
            "[epoch:110, iteration:27750] test_loss : 0.5534 test accuracy : 0.9038\n",
            "[epoch:111, iteration:27800] train loss : 0.0083 train accuracy : 0.9950\n",
            "[epoch:111, iteration:28000] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:111, iteration:28000] test_loss : 0.5160 test accuracy : 0.9074\n",
            "[epoch:112, iteration:28200] train loss : 0.0486 train accuracy : 0.9850\n",
            "[epoch:112, iteration:28250] test_loss : 0.5194 test accuracy : 0.9104\n",
            "[epoch:113, iteration:28400] train loss : 0.0202 train accuracy : 0.9900\n",
            "[epoch:113, iteration:28500] test_loss : 0.5062 test accuracy : 0.9132\n",
            "[epoch:114, iteration:28600] train loss : 0.0223 train accuracy : 0.9900\n",
            "[epoch:114, iteration:28750] test_loss : 0.5606 test accuracy : 0.9087\n",
            "[epoch:115, iteration:28800] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:115, iteration:29000] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:115, iteration:29000] test_loss : 0.5373 test accuracy : 0.9113\n",
            "[epoch:116, iteration:29200] train loss : 0.0367 train accuracy : 0.9900\n",
            "[epoch:116, iteration:29250] test_loss : 0.5645 test accuracy : 0.9086\n",
            "[epoch:117, iteration:29400] train loss : 0.0211 train accuracy : 0.9950\n",
            "[epoch:117, iteration:29500] test_loss : 0.5426 test accuracy : 0.9108\n",
            "[epoch:118, iteration:29600] train loss : 0.0211 train accuracy : 0.9900\n",
            "[epoch:118, iteration:29750] test_loss : 0.5330 test accuracy : 0.9091\n",
            "[epoch:119, iteration:29800] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:119, iteration:30000] train loss : 0.0066 train accuracy : 1.0000\n",
            "[epoch:119, iteration:30000] test_loss : 0.5405 test accuracy : 0.9109\n",
            "[epoch:120, iteration:30200] train loss : 0.0177 train accuracy : 0.9900\n",
            "[epoch:120, iteration:30250] test_loss : 0.5093 test accuracy : 0.9121\n",
            "[epoch:121, iteration:30400] train loss : 0.0139 train accuracy : 0.9950\n",
            "[epoch:121, iteration:30500] test_loss : 0.5451 test accuracy : 0.9104\n",
            "[epoch:122, iteration:30600] train loss : 0.0160 train accuracy : 0.9950\n",
            "[epoch:122, iteration:30750] test_loss : 0.5453 test accuracy : 0.9079\n",
            "[epoch:123, iteration:30800] train loss : 0.0156 train accuracy : 0.9950\n",
            "[epoch:123, iteration:31000] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:123, iteration:31000] test_loss : 0.5520 test accuracy : 0.9101\n",
            "[epoch:124, iteration:31200] train loss : 0.0319 train accuracy : 0.9850\n",
            "[epoch:124, iteration:31250] test_loss : 0.5251 test accuracy : 0.9129\n",
            "[epoch:125, iteration:31400] train loss : 0.0080 train accuracy : 0.9950\n",
            "[epoch:125, iteration:31500] test_loss : 0.5461 test accuracy : 0.9096\n",
            "[epoch:126, iteration:31600] train loss : 0.0122 train accuracy : 0.9950\n",
            "[epoch:126, iteration:31750] test_loss : 0.5618 test accuracy : 0.9069\n",
            "[epoch:127, iteration:31800] train loss : 0.0073 train accuracy : 0.9950\n",
            "[epoch:127, iteration:32000] train loss : 0.0099 train accuracy : 0.9950\n",
            "[epoch:127, iteration:32000] test_loss : 0.5465 test accuracy : 0.9077\n",
            "[epoch:128, iteration:32200] train loss : 0.0074 train accuracy : 0.9950\n",
            "[epoch:128, iteration:32250] test_loss : 0.5570 test accuracy : 0.9091\n",
            "[epoch:129, iteration:32400] train loss : 0.0236 train accuracy : 0.9950\n",
            "[epoch:129, iteration:32500] test_loss : 0.5440 test accuracy : 0.9105\n",
            "[epoch:130, iteration:32600] train loss : 0.0061 train accuracy : 0.9950\n",
            "[epoch:130, iteration:32750] test_loss : 0.5340 test accuracy : 0.9111\n",
            "[epoch:131, iteration:32800] train loss : 0.0078 train accuracy : 1.0000\n",
            "[epoch:131, iteration:33000] train loss : 0.0179 train accuracy : 0.9950\n",
            "[epoch:131, iteration:33000] test_loss : 0.5494 test accuracy : 0.9115\n",
            "[epoch:132, iteration:33200] train loss : 0.0578 train accuracy : 0.9800\n",
            "[epoch:132, iteration:33250] test_loss : 0.5089 test accuracy : 0.9128\n",
            "[epoch:133, iteration:33400] train loss : 0.0445 train accuracy : 0.9850\n",
            "[epoch:133, iteration:33500] test_loss : 0.5459 test accuracy : 0.9138\n",
            "[epoch:134, iteration:33600] train loss : 0.0048 train accuracy : 0.9950\n",
            "[epoch:134, iteration:33750] test_loss : 0.5140 test accuracy : 0.9134\n",
            "[epoch:135, iteration:33800] train loss : 0.0029 train accuracy : 1.0000\n",
            "[epoch:135, iteration:34000] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:135, iteration:34000] test_loss : 0.5642 test accuracy : 0.9094\n",
            "[epoch:136, iteration:34200] train loss : 0.0112 train accuracy : 0.9950\n",
            "[epoch:136, iteration:34250] test_loss : 0.5318 test accuracy : 0.9115\n",
            "[epoch:137, iteration:34400] train loss : 0.0061 train accuracy : 1.0000\n",
            "[epoch:137, iteration:34500] test_loss : 0.5345 test accuracy : 0.9145\n",
            "[epoch:138, iteration:34600] train loss : 0.0133 train accuracy : 0.9950\n",
            "[epoch:138, iteration:34750] test_loss : 0.5406 test accuracy : 0.9096\n",
            "[epoch:139, iteration:34800] train loss : 0.0168 train accuracy : 0.9900\n",
            "[epoch:139, iteration:35000] train loss : 0.0096 train accuracy : 0.9950\n",
            "[epoch:139, iteration:35000] test_loss : 0.5483 test accuracy : 0.9098\n",
            "[epoch:140, iteration:35200] train loss : 0.0067 train accuracy : 1.0000\n",
            "[epoch:140, iteration:35250] test_loss : 0.5623 test accuracy : 0.9078\n",
            "[epoch:141, iteration:35400] train loss : 0.0093 train accuracy : 0.9950\n",
            "[epoch:141, iteration:35500] test_loss : 0.5490 test accuracy : 0.9127\n",
            "[epoch:142, iteration:35600] train loss : 0.0236 train accuracy : 0.9900\n",
            "[epoch:142, iteration:35750] test_loss : 0.5514 test accuracy : 0.9116\n",
            "[epoch:143, iteration:35800] train loss : 0.0085 train accuracy : 0.9900\n",
            "[epoch:143, iteration:36000] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:143, iteration:36000] test_loss : 0.5441 test accuracy : 0.9105\n",
            "[epoch:144, iteration:36200] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:144, iteration:36250] test_loss : 0.5549 test accuracy : 0.9123\n",
            "[epoch:145, iteration:36400] train loss : 0.0032 train accuracy : 1.0000\n",
            "[epoch:145, iteration:36500] test_loss : 0.5688 test accuracy : 0.9121\n",
            "[epoch:146, iteration:36600] train loss : 0.0123 train accuracy : 0.9950\n",
            "[epoch:146, iteration:36750] test_loss : 0.5750 test accuracy : 0.9092\n",
            "[epoch:147, iteration:36800] train loss : 0.0083 train accuracy : 0.9950\n",
            "[epoch:147, iteration:37000] train loss : 0.0041 train accuracy : 1.0000\n",
            "[epoch:147, iteration:37000] test_loss : 0.5165 test accuracy : 0.9122\n",
            "[epoch:148, iteration:37200] train loss : 0.0182 train accuracy : 0.9900\n",
            "[epoch:148, iteration:37250] test_loss : 0.5454 test accuracy : 0.9093\n",
            "[epoch:149, iteration:37400] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:149, iteration:37500] test_loss : 0.5264 test accuracy : 0.9131\n",
            "[epoch:150, iteration:37600] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:150, iteration:37750] test_loss : 0.5212 test accuracy : 0.9140\n",
            "[epoch:151, iteration:37800] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:151, iteration:38000] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:151, iteration:38000] test_loss : 0.5293 test accuracy : 0.9109\n",
            "[epoch:152, iteration:38200] train loss : 0.0094 train accuracy : 0.9950\n",
            "[epoch:152, iteration:38250] test_loss : 0.5009 test accuracy : 0.9184\n",
            "checkpoint is saved !\n",
            "[epoch:153, iteration:38400] train loss : 0.0007 train accuracy : 1.0000\n",
            "[epoch:153, iteration:38500] test_loss : 0.4984 test accuracy : 0.9171\n",
            "[epoch:154, iteration:38600] train loss : 0.0062 train accuracy : 1.0000\n",
            "[epoch:154, iteration:38750] test_loss : 0.5283 test accuracy : 0.9153\n",
            "[epoch:155, iteration:38800] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:155, iteration:39000] train loss : 0.0503 train accuracy : 0.9900\n",
            "[epoch:155, iteration:39000] test_loss : 0.5296 test accuracy : 0.9103\n",
            "[epoch:156, iteration:39200] train loss : 0.0131 train accuracy : 0.9950\n",
            "[epoch:156, iteration:39250] test_loss : 0.5446 test accuracy : 0.9091\n",
            "[epoch:157, iteration:39400] train loss : 0.0156 train accuracy : 0.9950\n",
            "[epoch:157, iteration:39500] test_loss : 0.5266 test accuracy : 0.9135\n",
            "[epoch:158, iteration:39600] train loss : 0.0165 train accuracy : 0.9900\n",
            "[epoch:158, iteration:39750] test_loss : 0.5353 test accuracy : 0.9103\n",
            "[epoch:159, iteration:39800] train loss : 0.0142 train accuracy : 0.9950\n",
            "[epoch:159, iteration:40000] train loss : 0.0083 train accuracy : 0.9950\n",
            "[epoch:159, iteration:40000] test_loss : 0.5493 test accuracy : 0.9135\n",
            "[epoch:160, iteration:40200] train loss : 0.0146 train accuracy : 0.9950\n",
            "[epoch:160, iteration:40250] test_loss : 0.5183 test accuracy : 0.9175\n",
            "[epoch:161, iteration:40400] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:161, iteration:40500] test_loss : 0.5503 test accuracy : 0.9106\n",
            "[epoch:162, iteration:40600] train loss : 0.0121 train accuracy : 0.9950\n",
            "[epoch:162, iteration:40750] test_loss : 0.5989 test accuracy : 0.9049\n",
            "[epoch:163, iteration:40800] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:163, iteration:41000] train loss : 0.0097 train accuracy : 0.9950\n",
            "[epoch:163, iteration:41000] test_loss : 0.5360 test accuracy : 0.9125\n",
            "[epoch:164, iteration:41200] train loss : 0.0134 train accuracy : 0.9900\n",
            "[epoch:164, iteration:41250] test_loss : 0.5161 test accuracy : 0.9125\n",
            "[epoch:165, iteration:41400] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:165, iteration:41500] test_loss : 0.5448 test accuracy : 0.9098\n",
            "[epoch:166, iteration:41600] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:166, iteration:41750] test_loss : 0.5942 test accuracy : 0.9058\n",
            "[epoch:167, iteration:41800] train loss : 0.0165 train accuracy : 0.9950\n",
            "[epoch:167, iteration:42000] train loss : 0.0229 train accuracy : 0.9850\n",
            "[epoch:167, iteration:42000] test_loss : 0.5699 test accuracy : 0.9116\n",
            "[epoch:168, iteration:42200] train loss : 0.0027 train accuracy : 1.0000\n",
            "[epoch:168, iteration:42250] test_loss : 0.5578 test accuracy : 0.9119\n",
            "[epoch:169, iteration:42400] train loss : 0.0081 train accuracy : 0.9950\n",
            "[epoch:169, iteration:42500] test_loss : 0.5550 test accuracy : 0.9153\n",
            "[epoch:170, iteration:42600] train loss : 0.0130 train accuracy : 0.9900\n",
            "[epoch:170, iteration:42750] test_loss : 0.5519 test accuracy : 0.9128\n",
            "[epoch:171, iteration:42800] train loss : 0.0114 train accuracy : 0.9950\n",
            "[epoch:171, iteration:43000] train loss : 0.0052 train accuracy : 1.0000\n",
            "[epoch:171, iteration:43000] test_loss : 0.5731 test accuracy : 0.9086\n",
            "[epoch:172, iteration:43200] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:172, iteration:43250] test_loss : 0.5422 test accuracy : 0.9127\n",
            "[epoch:173, iteration:43400] train loss : 0.0028 train accuracy : 1.0000\n",
            "[epoch:173, iteration:43500] test_loss : 0.5501 test accuracy : 0.9122\n",
            "[epoch:174, iteration:43600] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:174, iteration:43750] test_loss : 0.5472 test accuracy : 0.9101\n",
            "[epoch:175, iteration:43800] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:175, iteration:44000] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:175, iteration:44000] test_loss : 0.5534 test accuracy : 0.9120\n",
            "[epoch:176, iteration:44200] train loss : 0.0171 train accuracy : 0.9950\n",
            "[epoch:176, iteration:44250] test_loss : 0.5514 test accuracy : 0.9129\n",
            "[epoch:177, iteration:44400] train loss : 0.0221 train accuracy : 0.9900\n",
            "[epoch:177, iteration:44500] test_loss : 0.5265 test accuracy : 0.9146\n",
            "[epoch:178, iteration:44600] train loss : 0.0147 train accuracy : 0.9950\n",
            "[epoch:178, iteration:44750] test_loss : 0.5268 test accuracy : 0.9143\n",
            "[epoch:179, iteration:44800] train loss : 0.0066 train accuracy : 1.0000\n",
            "[epoch:179, iteration:45000] train loss : 0.0075 train accuracy : 0.9950\n",
            "[epoch:179, iteration:45000] test_loss : 0.5461 test accuracy : 0.9153\n",
            "[epoch:180, iteration:45200] train loss : 0.0065 train accuracy : 1.0000\n",
            "[epoch:180, iteration:45250] test_loss : 0.5546 test accuracy : 0.9135\n",
            "[epoch:181, iteration:45400] train loss : 0.0058 train accuracy : 1.0000\n",
            "[epoch:181, iteration:45500] test_loss : 0.5424 test accuracy : 0.9130\n",
            "[epoch:182, iteration:45600] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:182, iteration:45750] test_loss : 0.5423 test accuracy : 0.9146\n",
            "[epoch:183, iteration:45800] train loss : 0.0183 train accuracy : 0.9950\n",
            "[epoch:183, iteration:46000] train loss : 0.0247 train accuracy : 0.9850\n",
            "[epoch:183, iteration:46000] test_loss : 0.5379 test accuracy : 0.9135\n",
            "[epoch:184, iteration:46200] train loss : 0.0285 train accuracy : 0.9900\n",
            "[epoch:184, iteration:46250] test_loss : 0.5375 test accuracy : 0.9132\n",
            "[epoch:185, iteration:46400] train loss : 0.0317 train accuracy : 0.9950\n",
            "[epoch:185, iteration:46500] test_loss : 0.5616 test accuracy : 0.9128\n",
            "[epoch:186, iteration:46600] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:186, iteration:46750] test_loss : 0.5598 test accuracy : 0.9094\n",
            "[epoch:187, iteration:46800] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:187, iteration:47000] train loss : 0.0019 train accuracy : 1.0000\n",
            "[epoch:187, iteration:47000] test_loss : 0.5693 test accuracy : 0.9138\n",
            "[epoch:188, iteration:47200] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:188, iteration:47250] test_loss : 0.5507 test accuracy : 0.9155\n",
            "[epoch:189, iteration:47400] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:189, iteration:47500] test_loss : 0.5532 test accuracy : 0.9137\n",
            "[epoch:190, iteration:47600] train loss : 0.0074 train accuracy : 0.9950\n",
            "[epoch:190, iteration:47750] test_loss : 0.5470 test accuracy : 0.9141\n",
            "[epoch:191, iteration:47800] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:191, iteration:48000] train loss : 0.0087 train accuracy : 0.9950\n",
            "[epoch:191, iteration:48000] test_loss : 0.5176 test accuracy : 0.9138\n",
            "[epoch:192, iteration:48200] train loss : 0.0147 train accuracy : 0.9950\n",
            "[epoch:192, iteration:48250] test_loss : 0.5335 test accuracy : 0.9129\n",
            "[epoch:193, iteration:48400] train loss : 0.0028 train accuracy : 1.0000\n",
            "[epoch:193, iteration:48500] test_loss : 0.5658 test accuracy : 0.9109\n",
            "[epoch:194, iteration:48600] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:194, iteration:48750] test_loss : 0.5590 test accuracy : 0.9144\n",
            "[epoch:195, iteration:48800] train loss : 0.0037 train accuracy : 1.0000\n",
            "[epoch:195, iteration:49000] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:195, iteration:49000] test_loss : 0.5486 test accuracy : 0.9142\n",
            "[epoch:196, iteration:49200] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:196, iteration:49250] test_loss : 0.5448 test accuracy : 0.9146\n",
            "[epoch:197, iteration:49400] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:197, iteration:49500] test_loss : 0.5405 test accuracy : 0.9141\n",
            "[epoch:198, iteration:49600] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:198, iteration:49750] test_loss : 0.5371 test accuracy : 0.9169\n",
            "[epoch:199, iteration:49800] train loss : 0.0202 train accuracy : 0.9900\n",
            "[epoch:199, iteration:50000] train loss : 0.0077 train accuracy : 0.9950\n",
            "[epoch:199, iteration:50000] test_loss : 0.5336 test accuracy : 0.9149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECu3yS0OvfoR",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Visualize and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sqVp-vLRy",
        "colab_type": "code",
        "outputId": "652b4e57-0ce5-40c8-f8b3-042c3002f39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "\n",
        "training_process = False\n",
        "\n",
        "if not training_process:\n",
        "  # Re-load trained model\n",
        "  my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "  optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "  # Testing\n",
        "  n = 0.\n",
        "  test_loss = 0.\n",
        "  test_acc = 0.\n",
        "  my_classifier.eval()\n",
        "  for test_inputs, test_labels in test_dataloader:\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    logits = my_classifier(test_inputs)\n",
        "    test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "    test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "    n += test_inputs.size(0)\n",
        "\n",
        "  test_loss /= n\n",
        "  test_acc /= n\n",
        "  print('Test_loss : {:.4f}, Test accuracy : {:.4f}'.format(test_loss, test_acc))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_loss : 0.5307, Test accuracy : 0.9130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXl4lNXZh++TPSEhOwkhCQn7vi8q\nKqDWD8Sl1qWuVauirdb2a2vVr9aq3bRaa7Vad+uK+4KKAiIIKiBhJyxJCNlD9n1P5nx/nHkzk2SS\nDJCAGZ/7unJN5p13OfPOzO8853eec47SWiMIgiB4Fl4nugCCIAhC3yPiLgiC4IGIuAuCIHggIu6C\nIAgeiIi7IAiCByLiLgiC4IGIuAuCIHggIu6CIAgeiIi7IAiCB+Jzoi4cFRWlk5KSTtTlBUEQBiRb\nt24t1VpH97bfCRP3pKQkUlJSTtTlBUEQBiRKqWx39hNbRhAEwQMRcRcEQfBARNwFQRA8EBF3QRAE\nD8QtcVdKLVJKHVBKZSil7uxmn0uVUnuVUqlKqdf7tpiCIAjCkdBrtoxSyht4AvgBkAdsUUot11rv\nddpnNHAXME9rXaGUGtJfBRYEQRB6x53IfQ6QobXO1Fo3A28AF3Ta50bgCa11BYDWurhviykIgiAc\nCe6I+zAg1+l5nn2bM2OAMUqpr5VSm5RSi/qqgIIgfI84vAeyvznRpfAI+moQkw8wGlgAxAPrlVKT\ntdaVzjsppZYCSwESExP76NKCIHgMa+6Dyhy4ZfOJLsmAx53IPR9IcHoeb9/mTB6wXGvdorU+BKRh\nxL4DWutntNaztNazoqN7HT0rCML3jZpCqCvtun37q/DaJce/PAMYd8R9CzBaKZWslPIDLgOWd9rn\nA0zUjlIqCmPTZPZhOQVB+D5QWwwNFaB1x+2H1kP6KtfCL7ikV3HXWrcCtwIrgX3AW1rrVKXU/Uqp\n8+27rQTKlFJ7gbXA7Vrrsv4qtCAIHoitDepKQLdBU3XH12qLzGPhzuNfrgGKW5671noFsKLTtnuc\n/tfAr+1/giAIR05dKWib+b++HAJCHa/V2hPwDu+GUWce/7INQGSEqiAI3w1qDzv+byjv9Jo9cj+8\n6/iVZ4Aj4i4IwneDWqfhMfUVjv/bWqDe7vIWiri7i4i7IAju0VwHX/wFakv65/xWdA4dI/c6+/VC\nhkJZBjTV9s/1PQwRd0EQ3OPT38H6v5uslf6gxtmWcYrcLdEfdSagoXgvXXhxCWx6qn/KNUARcRcE\noXd2v2NyzcFhkfQ1tcXgF2y/RnnH7QCjzjKP+ds6HlddANlfwcEv+qdcAAXb4eNfm4yeAYKIuzAw\nKTkAn9/XNR96oNBcN7DKvv5hGDoVvP2gvp9yzWuLjPUSENrRlrEi97gZED0e9rzb8bg8+3KdZRn9\nUy6t4dM7IOV5k63THc11sP4haKzufp/jiIi7MDDZ/ip89UjHpvyJoq4Mtr/mflTXXA+PTIBtLx3d\n9RoqOka27qA1/GcebHvZ9eutzXDgU9cVTvF+KNkH06+GoMh+jNyLICQWAiM6Re52cQ+OgelXQd63\npnK3yNtiHiuyTOdrX9HWaj7brK8g1z4dQvbX3e+fvhq++DOsvKvvynAMiLgLAxPrx12Vd2LLkfst\nPHUqfPhzOLi2+/1am+H1y6BgB5Tsh8bKnvfviTeuhDeuOLJjmqqhaI8RKldsfRGWXQaZ67q+tvcD\nQMH48+3ifoQVi7vUFkHwEAiK6BS5F5to3jcApvwYvHxg+yuO1/O3mkfdBhVurR3tHpuegIdGwJtX\nmoolLLH7+wdQftA8bn/VVJQnGBF3YWBSst88VuX2vF9PfP0vePPqYyvHR78CZf8Z9ZSDXZkNaZ9C\n6ntQvM9s6+wdu0NdqZk1MWcj1BTB/hWm1dAbVoZL2UHXr6e+bx53v+P6teHzICTGCG9/Re41RUZE\nAyO6dqgGx5j/g6NhzCLTAkn/3ETXBdth2Czzel9aM9nfmMps8DA48x4YscBE7t210MozISgKhkyE\nz+464f68iLsw8GiuM2IJxxa5p600EVZb69Ed39YKpWkw+WIITTCRsc0Gu97uag9Y6XwFO4zFAVCV\n0zG3uzucrZL01YD9+b7l8NFt8MlvoKmm53NY1ka5C3GvLoScTeATYM7Z0uh47fAeU5FO/KF5HhTV\nP+LeVAstdXZxD+/aoWqJO8CZfzTPX7sIXrsYWuph6mXmtc7ifmi9qQBdCW1NEbx4Tve584U7YeSZ\n8PONxg5KOg0aqyBjDez7uKuFVZYJUaNh/u1QcQjSPjvy+9CHiLgLx5+SNHhkIlQeZdRdmub4/1jE\nvTQNbC2OiuJIqcw2x0eNgZhJRgjTPoX3brCLsBOWuBfuhKK9pmMSHJZCd+z9EB4a5ehbSPvMCFto\nIqy535y3tQH2f+I4pqm2q/DU2SsRV379vuWAhrPuNfbNJ7+Gl843HvaW54zoT/yR2Tcosm8n72qu\nN4/OvnqQq8jdaXG36DFw81dw+u8ga4PZNvoHpmxl6SaSL0039/rVi+CNy+GJucY/d2bd30wkvunJ\nruWqLTEzVA6d4tg2fJ55fP0SY9V8+0zHY8oPQsRIGHee+Xw2ujjvcUTEXTj+5GyE6jwoOApbAhx+\nu++gIxf3Hcvgm38bgbME92ib8lYlEzUGYicZYdlrnzC1IqvjvpYgNlaa5v6Y/wHl3bO4VxfC8ttM\ndkr+VtMaOPgFjD4bxi4yQjxkAoQNh11vOo55eLRdsJ1wbiGUH3L8r7WxYqLHw+wbYVA07HjNRLzv\n/8ycd/IlMCjS7B8Uad7D0bZ2nMn+Bh5IMB221fZZxAcPNbZMU7Wj9dM5cgfw8Yczfg83bYAfPQfh\nSRA5CrI3wguL4cmT4NWLTUtj8d/NZ5Pp1MdRcsBYO76DIPWDrhkuh+0TlA2d6tgWOgwmXACTLjYR\n/aq7TUUCpkKtLYLIEeDtA3OXmvTMPe8d+306SkTcheNPuX026Mqcozu+eB94+ULi3J49d607Nrnb\nWmH1PfDlgw7fG0yUdzRYx0WNMpG7tjnS9Dq/N+dot7XBpPXFTHAt7sX74eUfwvM/gNYm+7a9JmOj\nqdpUDOPONdtPvsWIb+Y6YzMcWGFsisN7TCfuv2eb6N959KezNbPvI5N9MuunRpQufwOu+QjOeQhy\nvjHnmnuTY/8gu8jXFMJrlx5Zv8G+jzvuv3MZ2OyeudUXEDHCRO5govemWmiu7Ri5OxMzAabY53mP\nHAWlB8znMP48Y1Vd9BzMvM60lJzTGDc8Ar5BcMmL5vNI7STC1vcmdnLH7Ze+DBc/b87rPxi+etRs\nt77TESPM48xrYdhMeOc6+PKhE5L2KuIu9B2pH7gW7MYqIzQWlrj0JO6tzaazM3dL19dKDhhvMzyp\n58h9z7vw9GkmmgMTudUVG4Hc+4HZpry7Ru4tje61CErTTKQbGO4QAZs92uxc6dSVgF+IqZQAhow3\nP/68rUbAbW2Oa6Z9asoaNcYISdhwUxlZ6XjD58GI+XDDFzDtSuM3a5uJRC2ftyrPlKE0DQ5tMNFv\nYDigHELaXA8r/89UTLN+arbFz4Lk083zxFNg5BkdBc6K4LO+gvSVsPON3u+Tda13bzCVK5iofN/H\n5v+yDPOd8PaHwfH2cmJaV9Z9HBzf+zUiR5nHebfBJf+Fu/IgaR74+EH0WNMnAkZoM9fC2MWmFRQ9\nDlb9AZ46zVGmwp0mO8YqS2eCIiBhjqMVaX2nI0aaR/8QuHYFTL4U1v7ZjO612dy6VX2FiLvQN7Q0\nwtvXwroHO26vL4cnTjIia4m5ZQu4EnerM68801gLH9zsiF4tSvabH2tovEmZa65zXSbLEz30pXnc\nucw0w8HYDd7+EDe9q7h/9U/495zeB6OUZRgBBghPdpw7dkpXH7+uxFgOQ8ab50PGmyZ+U5Up18rf\nw+MzzTUrso01cfV7MG6JsV6K95mKLnK0I7KNnwlKmYpu1A9g81OQaX+v1XmO+1uWYcQ9NMH8WUK0\n6m4jnuc8ZKJ2Z7y8TQR/xdsdt1uRu5Xv7e56p5nrTISc+y20NBiv3Ep3LMswnZERyeDl5RDUhgpH\nVlT0mN6vMf48k4t/6v+a587vKXaKI3IvzzQtmeEnm/u36G9m9GtbM7z1E/jmcdPCiJ3S9RrORI02\n97KttWvkDiZ188Kn4eRbzXfxtYv7b14eF4i4C31DVS6gzY/YuQm68vfmh1RdAM/9wIh9d7ZM2UF4\nINEIlOXBlmWYlEWL1mbjZ0eNMUIFUNV51UdMs9p54Eljlel0nHaFOa6xyvw4o8d2tWWyNpjMjYzV\nXc/rTGmaI1r08oK4aTB0GiTMdW3LBEVB/GyTsx2aCCMWGnvmi7/At09Da6N5vxVZED7cceyQ8eZa\nuZtNtOiKU39lvPm2JgiONfekvTI9aFoswUOMJ1yeaTJ6Up6HU34Bw09xfU5vn66i3y7udlEv2tOx\n87M7Dtg7fNuajMDv/dBMNZB8uvncrc5IcFRe9WWm8x1lKrXeiBoNF/wb/AZ1fS1mkvke1hQ5yp5o\nf98jzzD2zA1rzL1YdbfJZIqf1cv1xpgKoTLbVE7BseAf3HEfLy84+8+w5BHT2nnqVNOfcRwQcf++\nUZHde9qcM/s/cTRVe8KKVKvzHJFw1tew83UTSV32upmve8drxsf1DTLi41wRpL5vfvyFO01lAJBw\nEqz9q/FItbaLvjZN5lB7U92V757yPPgEwtTLTcS7/VUjntMuh8STzD5RoyFypCmXdU/aWh2+sHMG\nioXWJrp7/2YjPlFOEeWPnoHLXjNla6wyfxZ1JTAoCs64G3660vzolYLTf2uE19vf7Fd20NzLMGdx\nn2C86Yby7gVn+Dxj8/iHmrTF6nzHZ1KZa/6CY0xkWbgT3l8KiSebtMIjISjKPJYfNJYWGnJ6Wcza\n1gYHPoPR9k7kPe+YymXcuRAz2W7LHDIVDxi7DUzUXrLf3E+/oCMrZ2csa6lot+nQD4o0Fbsz/sHw\nkw/hZ9/AtZ/A3Jt7Pqf12ZemmQ5b56jdGaVg9vVw4xpj17x0fvcjhfsQEffvE1qbTrov/+76dVub\n6WCz2YzIfXiLGQn5znW9j/xzjlStkZeZa82P+bTfmIgoIAxSXjCvDT/FdJQ5R337PnKcyxL3K96E\nST+CNfeZisHypUPjncTdhT+e8QWMORvGnmPsgLV/NeI3bKYRNYCosY6I0KqQivaY/QcNgbRVXS2h\nzLUm6ty5zH4OJ3G3yhRmb1E4p3rWlxp/PijCYc0AjFlsmu0/fgVQRigqcztF7uMc/8fP7vpewQjI\nxS8YKyc82VRkViYH2lw/eIg5XmuYs9R0nnr7uj5fd1hRNZhZGr39HBZNRVbXQVA1Rcbmqi+FqT+G\nYTOMsLU2wvzfmcq1tcFU6lbkHhhufPCcTeZ+dBbhoyF2knk8vMeUN9FuyXTGyxtiJkLSqeAb2PM5\no+zfnfxt5i9+Zi9lmAxL18Gs68z5+xkR9+8TDRWmaWrZIp05+AW8eZWZ0jV9lYl2Z11vBPqLP/V8\n7soc01kYlugYwl6019gWfkHmRzNivuPaIxbYj3OKLgt3OM5VnW8ENjAMLnreVAz5Wx1RemiCmWRK\neTksHIumWtOsjpnssByaa2HOTY5rK28jNNYP9NM7TZqkNU/JgjugucY0oRur4aXzYMXtZgKtkKGm\nqT1oSMdUOYuwRMf7AFNR1pcbce+Mlxf8z19MnnZYgmm621o6Ru6Ro015fQeZKL47wpNMZB86zDzP\n/dZ04loMGgJTLoO7cmHxg+beHik+/o5zxk4xlWXWBlNhvHsDvHt9x76KZ88w353YKabzMvl0s33G\n1UbYLVsLzHOLxJNM+UvT+0bcA8PNd2bri6YS6s6KOtJzDhpi5giytZiWSW/4B8O5/+w+yu9DRNz7\nitZmkz/d0nB8rnd4T+/N4c5YQtrdZFtWBHxwjfnzHQSLHoCTfw673+55RrzKHBO1jjzD/NjbWqE4\n1aSqWYxYaB69fBwDQiqyTebEDvsQ+ujxjsh9cJzZppT54ZcddETDg4eZqDM8uesIQyv/PHqssUKi\nxhphtUZZRo6EX+81YhM1xkSxtUWm83bTf4x3Ou0q03T/8FYz50rWV6ZTLPtrE2mf8gv4bZoZkt8Z\nS5gtcW8oB7QpS09EjjJpieCwJsB0zEWPhYTZppLsjcF2cW+uheTTHNuDh5jKxJUnfSRY0Xv0WNPh\nW7AdXr/UUTFafRh1ZcamW/h7uGm9sSQmXWz6JObfafZxFvcIZ3E/2XQ2tzWZz68vmPJjk1UUN8OU\nuy+IGmO+O/6hDrvvO4KIe19xcA2s+n3/LWTQmVV3m8mqjgRrYE134m7lQmesMVF88mkmjeykW+zb\nPzeP9eVdh9dXZJuIdcRCk2qYtd5cb8hExz4jFpjHsERH5LL3Q3hopBktOGym2acq10TjlkiBEYGy\ng+a14BgjeGDKmP11x0E17eJutzPO+5dJjfPxd+wTEmsqDS9vky1yy2YT6ZcfNCLqG2CyRXzstsOS\nR0yO88QfmRxmcN2sB1MpWH0KZQcdA4jcEXeb/X04izuYa5//756Pt7A6msG0LKzsk84DgY4W631E\njTHfjYk/Mt976/MqtacHltlFfug0x72KmQDXrzKZQ2AqcN8g0z8SMtRxjYS5jv/7InIHOPMP8Kvd\nsHRt1/t7tFhZPCMXHrnF1c+IuPcV1mCUvpyVrifKM00n1JFMcWqVrfaw65xbS/TLD5rzjzzDPB8U\naX4MBdvNPv8YBw8Mh4//13FsZY7xiZNPB5SJgKGjvxyRbAQsepyxBPxDzeARvxAjvle/b4S/udZ4\n4FbkDiaqq84zUaHltYO5XlO16SS0KNlvWgcRyeb58JN79zh9/OGiZ01rZcQCsy1mIiz90oj8rOtM\n6uIlL3bNiOiMUuZ9bP0vPD7DpNaBa1vGmfYoVnV8j2Dso7CELoe4ZFCUo4M2LNEREXc3EOhIsTJm\nokablsCFT5mpAK54y3jwVvqi8yCv7rBaZREjzLkswpNMCwo69mt817DKNvrsE1sOF/j0vovgFlaG\nxdHOU3IktLUYC0W32dMC3UgTA0fkbms1mR7BncSmtsiMumuye6aWuIPJB8/favz0tiaTkpfygskr\nHjLeZHyEJZome9w0RwsmppNHfOU7jgg6LNF0YP7oaYf4Wn51W3NHcbf82PwUx+hMMJM5gcllD442\naYYlB4xQHmkkNWQ8/PaAYzUgMO/H8omPhMhRpgXhH2qyQ8ANcbe/x8FxHVsZR4pS5hwVh8z9jBxl\n7ltfiXvESOP9W/aONRUAmGtZA3tK04zYO/cfuOKMe2ifDM35PSSdanz3o+kbOF6MPtu0aMedc6JL\n0gUR975Aa8c8KccjcreEHewDadwUd+eKp6awq7jXHDbZFCUH7BGVU8QVN92kKqa+b5r5P37FLDix\n9UU4+RdmH+tHPGKhifJ9B0FYUsdrWNE0mJGELfUdo2rn6LSDLWMXPltrx32Chxih2f6qyQKKn2X8\neis74kjxD+l9H3dY/HeT9pj6vpnuABxphN1h3e/exNAdQuMd4p50qvk8AvpIJM+612S4uCJ6rJn5\nEkzkHjmq936CMd1EvYv/bnz37zKRI+Gqd3vf7wTgli2jlFqklDqglMpQSt3p4vVrlVIlSqkd9r8b\n+r6o32EqDplMFOV1fCL3CqeJnzqPrmys7jppVftxWQ4/1pXvbq2Es+Rh88Ny9pTjppvHtM9MtBwY\nDpMuMqlvRfaOVkuURto7ToeM69jU7syUSx3+tYUVuUNXW8YitJM9kXy6Pe/ay3Tmlh90+O0nitBh\npiUw6WLzXHl1P5S9/ZgEE+mG95G4K28IiTOZKbd+230fwZHiG9D9e4keZ75nLQ0mcnc38HDFoMjj\nklXiqfQq7kopb+AJYDEwAbhcKeUqH+tNrfU0+99zfVzOvkdr+Poxk4d7rFiWTNJpXQfm9AeWeHv5\ndBX3NffD0/M7zuUCJoe9MtfRUVVT2Ol1m2NRhLGLuzYznVP+Rsw3j7OuM5H3p/b63hLmhLnG2uht\n+LYrAsKMNQQdxT1gsEk7g67iPvNa44ffvMHRafhd8Wmjx5j7EBTVc0UHJsK94EmTjXOsTL8KFt7V\ndYRpfxM1BtAmDdYaSSycENyJ3OcAGVrrTK11M/AGcEH/Fus4UHIAVv/BjKA8VvK3md7+sYvN4Iza\nPqgwXLHlefj2WfOj8fYzWQidV9Y5tN5MyWql1FnUFJpcXEvcO5exvsxYHiGxrq8dEOqwDZLt4h43\nw0T4PgGm88sSVh9/uO5TY0scKUo5xNs5ewIc1+/c2ThkvMkmiRwJp/7abDuaiqW/WPx3kxfvDlMu\nOXpLyZmkU+H024/9PEeK1WJK+9RYh+5MGyD0C+5U68MA5/HdecBcF/tdpJQ6HUgD/ldrfQzrnx0H\nrME0JWk97+cOBdvMpP6WdVCRbUQy5QWTg33eo8d+DTBzrDRUmpnuwhKNv5mxxvF6XZkjDS3j845e\nthXtR40yUWTnyL3WbtP0lC6XeLKpACyRVcpMBzv7BtPJ6xyZDj0GcQ1LhJqCrkPOI0eYaWh7yhqZ\ne5NpWbgz0dTxYvjJ5u/7QORIU9lvfto8PxZbRjgm+ioV8iMgSWs9BVgNuFzWXSm1VCmVopRKKSk5\nfrOjucTyra0lz3Ys6z7/uydsNjOgKHaKwyu1Bq9sfMJ0ODovjnC0NFQYP7+pyqzyE57kmBdlzZ9M\nWmKOfWrbwHCzvqQzVkdveJKJiDu/V8ue6i5yBzOg6frPu3q3Xt6OvPO+YM4NsMDFCvITfmhyqnvq\nGFSqY/qlcHzx8Tfz64QMNfaa2DInDHci93zAOVSKt29rR2vtvH7Vc4DLyUu01s8AzwDMmjXr+M9e\n74xz5F520IxOPOU2OLuXYfa2NpOepbxMml9diRmmHjvZadh5FpRmOPzwPe+aCaKOhuyNJuXMeQ4W\nW4sZmWlF0BseNo+Fu0x+89ybzaCg6kLHYJFdb5qIPTTBCLgVuWtt3oM7kbt/cO853n3BqLPMX2dG\n/8D8Cd9tRp0FP19oJmM7Ht8XwSXuRO5bgNFKqWSllB9wGdBhDS+llLM5ej6wj+86lri31DkWHLAi\n357Y8y68uAheOBveu8lE7WDE3TfQdPpVZJuFDMBYNbvfdt3JWnLAsYYkmIqjKLXjYr7v3wQf/Mwx\n74o1hD88yTH6c/IlJlLKTzHzpYy1d4a+d6MZSJO20uSBn/5bk/sdEmMid61NxP+PsWYRYei7UYzC\n9xsv7+92fvr3gF4jd611q1LqVmAl4A28oLVOVUrdD6RorZcDtymlzgdagXLg2n4sc99QfsikidUU\nOKbfLNhuxLan6UXTV5nBKMnzzSyGEckm5cyyAsKHw+FdJmofMsFM9fnJb8wIyrhpkJdirrNvuen8\nXHAXLLBnm6z9q4nCBw0xkwsNP8WRWrnH20Tds64zMxNGjDD++c82Gu/922fhszvM/Baxk81MjLvf\nho9+aY4PTXCsthMy1HSovrcUdr9lWiEHPjHN6GOdWlUQhO8EbnnuWusVWusxWuuRWuu/2LfdYxd2\ntNZ3aa0naq2naq0Xaq3392ehj5m2FuOLj11kntceNkOqba2u17SsLYHNz5j5SzLXmeHp068yIzW3\nvWx8RWt60AkXGCHP2WjWupxwoRml+ObV8Mlv4bkzYcVvTdQeGOEYNl9qX5Ri5BlGYL96xBGtg9lv\n6FSzsvol/3UMd46ZYKKkmdeYtSKnXmF85zPvgV/ugp8sN5H8kn84jQwdbiZQSn0f5v3KkXonUbsg\neAzfzxGqlTkmTWvYTLMQRV2x8anX/tWIsvNMegBbnjWjDKvzjD898gwzq6FfsBmqH7vIse8pvzAp\nipufghk/MQMxrlkOr1xozjP7Bjjtt8b3fvtaE+UDfHanqSAufNrMPrjBvnILmFVkivaY83p5wcQL\nu74n38CuWTlKmcwRKy/dYsqlpoUxdJrJH6/KM52/PXWmCoIwoPDsicO++ItZebwzVqZMxAjHQghj\nzzETRWV/bayZz+8zK9C3NDoWn7CWexuxwMwWaM290jkvOfk0kzFgja6LmwY3fmHmVVnyD9PJqZRj\nNF9tsUldnLPUDKdPOs1UPikvmijbGsXpau7wo8HH34zqDLAPFgqNN3OKW7aNIAgDHs+N3KsLjbUR\nEgfzOw3mKHcS9/jZZv3DIRNMHveWZ+Fv8Y65W3a9YayaYTPNY/Q4x8jJMYuMd+7OgJmI5I7zqoDJ\nxdY2e4euduSlJ8wxWS8N5aaimH6V6fy0hvX3Byf9rP/OLQjCccdzxX3rf42HXpVj0gid58IozzRz\nSAfHmA7Neb80dsfptxu7oq7EpHO9/zNYfY8R+h/8yeSsO88zPeVSexQ8v8vl3cIazbfdvsTaMPsy\nXb6BRuCzNpho3TcQ5i49umsIgvC9xDPFvbXZCHFQpBlWX5TqiIptNjjwqWMBAW9f8A41r4XEGM/c\nYtrlsP4hM7th/GwzMtQZb1+YfPHRlzNylMlUKU0zvrplk4CxTbI2mHIKgiAcIZ7juWvtGIWZvtKk\n+p11r3lu5aIDZKw2nvvs63s/57QrzGPSqcZj72t8/M1gJOi68PHUy2HalY7FnAVBEI4AzxH3vR/A\nv6aaSbz2fWRsmKlXmJx057U/Nz9tJrma4MbcZxEjYPFDZpX2/sKyZhI6TdcTlgA/fFLyzgVBOCo8\nR9zTVgHaDOZJ+wzGLDbTncZMcsw3XnbQrHU666fur9Izd6lZAKK/sNaHTJjTf9cQBOF7h2d47lqb\n4fXgmMLXWt08drLJOW9rgZ3LjMc94ycnppyumHmNaV3IogSCIPQhAztyb6g0KwGVZUB1vvGowcyt\n3p6DPtmsx1mUalIORyx0TKb1XSA8CU7+ed+tkiMIgsBAj9x3vQmf/s4sGgFmPpWSAyYKtrzq4fNM\ntsuyy8xMiGfde6JKKwiCcNwY2OJuzexYsA0GxxtRv+5TY71YhA4zizm/fin4hThmTBQEQfBgBra4\nV2QZW0NrM0mXUq5TFkedCVe9Z5bAk+wTQRC+Bwxwcc822TAXvwBevWS/dJ48SxAEwYMZuB2qWjsi\ndx//3leWFwRB+B4xcBWxthiOGltvAAAgAElEQVRaG4y4C4IgCB0YuOJurVAUNvzElkMQBOE7yMAV\n94os8yiRuyAIQhcGvriHJZ7QYgiCIHwXGdjiHhIHvgEnuiSCIAjfOQawuGebhTUEQRCELgxgcc8S\nv10QBKEbBqa4N1ZDdZ5ZyUgQBEHowsAU9+K95jF28okthyAIwncUt8RdKbVIKXVAKZWhlLqzh/0u\nUkpppVQ/rm4BFNmXzYuZ2K+XEQRBGKj0Ku5KKW/gCWAxMAG4XCk1wcV+IcAvgc19XcguFKVCQCgM\nHtbvlxIEQRiIuBO5zwEytNaZWutm4A3A1QKkfwIeBBr7sHyuKUo1E4bJAheCIAgucUfchwG5Ts/z\n7NvaUUrNABK01p/0YdlcY7NB0V6xZARBEHrgmDtUlVJewCPAb9zYd6lSKkUplVJSUnJ0F6zKgeYa\nEXdBEIQecEfc84EEp+fx9m0WIcAkYJ1SKgs4CVjuqlNVa/2M1nqW1npWdHT00ZW4KNU8xkw6uuMF\nQRC+B7gj7luA0UqpZKWUH3AZsNx6UWtdpbWO0lonaa2TgE3A+VrrlH4pcVEqoCB6XL+cXhAEwRPo\ndSUmrXWrUupWYCXgDbygtU5VSt0PpGitl/d8hj7mlNtg3LngH3xcLysIgjCQUFrrE3LhWbNm6ZSU\n/gnuBUEQPBWl1Fatda9jiQbmCFVBEAShR0TcBUEQPBARd0EQBA9ExF0QBMEDEXEXBEHwQETcBUEQ\nPBARd0EQBA9ExF0QBMEDEXEXBEHwQETcBUEQPBARd0EQBA9ExF0QBMED6XVWSEEQhCOlpaWFvLw8\nGhv7f9VNTyUgIID4+Hh8fX2P6ngRd0EQ+py8vDxCQkJISkpCyVrHR4zWmrKyMvLy8khOTj6qc4gt\nIwhCn9PY2EhkZKQI+1GilCIyMvKYWj4i7oIg9Asi7MfGsd4/EXdBEDyOyspKnnzyyaM69pxzzqGy\nstLt/e+9914efvjho7pWfyLiLgiCx9GTuLe2tvZ47IoVKwgLC+uPYh1XRNwFQfA47rzzTg4ePMi0\nadO4/fbbWbduHaeddhrnn38+EyZMAOCHP/whM2fOZOLEiTzzzDPtxyYlJVFaWkpWVhbjx4/nxhtv\nZOLEiZx99tk0NDT0eN0dO3Zw0kknMWXKFC688EIqKioAeOyxx5gwYQJTpkzhsssuA+DLL79k2rRp\nTJs2jenTp1NTU9On90CyZQRB6Ffu+yiVvQXVfXrOCXGD+eN5E7t9/YEHHmDPnj3s2LEDgHXr1rFt\n2zb27NnTnn3ywgsvEBERQUNDA7Nnz+aiiy4iMjKyw3nS09NZtmwZzz77LJdeeinvvvsuV111VbfX\n/clPfsLjjz/O/Pnzueeee7jvvvt49NFHeeCBBzh06BD+/v7tls/DDz/ME088wbx586itrSUgIOBY\nb0sHJHIXBOF7wZw5czqkFT722GNMnTqVk046idzcXNLT07sck5yczLRp0wCYOXMmWVlZ3Z6/qqqK\nyspK5s+fD8A111zD+vXrAZgyZQpXXnklr776Kj4+JqaeN28ev/71r3nssceorKxs395XSOQuCEK/\n0lOEfTwZNGhQ+//r1q3j888/Z+PGjQQFBbFgwQKXaYf+/v7t/3t7e/dqy3THJ598wvr16/noo4/4\ny1/+wu7du7nzzjtZsmQJK1asYN68eaxcuZJx48Yd1fldIZG7IAgeR0hISI8edlVVFeHh4QQFBbF/\n/342bdp0zNcMDQ0lPDycDRs2APDKK68wf/58bDYbubm5LFy4kAcffJCqqipqa2s5ePAgkydP5o47\n7mD27Nns37//mMvgjETugiB4HJGRkcybN49JkyaxePFilixZ0uH1RYsW8dRTTzF+/HjGjh3LSSed\n1CfXfemll7j55pupr69nxIgRvPjii7S1tXHVVVdRVVWF1prbbruNsLAw/vCHP7B27Vq8vLyYOHEi\nixcv7pMyWCitde87KbUI+BfgDTyntX6g0+s3A7cAbUAtsFRrvbenc86aNUunpKQcbbkFQfgOs2/f\nPsaPH3+iizHgcXUflVJbtdazeju2V1tGKeUNPAEsBiYAlyulJnTa7XWt9WSt9TTg78Aj7hZeEARB\n6Hvc8dznABla60ytdTPwBnCB8w5aa+c8p0FA780BQRAEod9wx3MfBuQ6Pc8D5nbeSSl1C/BrwA84\no09KJwiCIBwVfZYto7V+Qms9ErgDuNvVPkqppUqpFKVUSklJSV9dWhAEQeiEO+KeDyQ4PY+3b+uO\nN4AfunpBa/2M1nqW1npWdHS0+6UUBEEQjgh3xH0LMFoplayU8gMuA5Y776CUGu30dAnQdaiXIAiC\ncNzoVdy11q3ArcBKYB/wltY6VSl1v1LqfPtutyqlUpVSOzC++zX9VmJBEIReOJYpfwEeffRR6uvr\nXb62YMECBkIat1ueu9Z6hdZ6jNZ6pNb6L/Zt92itl9v//6XWeqLWeprWeqHWOrU/Cy0IgtAT/Snu\nAwWZfkAQBI+j85S/AA899BCzZ89mypQp/PGPfwSgrq6OJUuWMHXqVCZNmsSbb77JY489RkFBAQsX\nLmThwoU9XmfZsmVMnjyZSZMmcccddwDQ1tbGtddey6RJk5g8eTL//Oc/AdfT/vYnMv2AIAj9y6d3\nwuHdfXvO2Mmw+IFuX+485e+qVatIT0/n22+/RWvN+eefz/r16ykpKSEuLo5PPvkEMHPOhIaG8sgj\nj7B27VqioqK6vUZBQQF33HEHW7duJTw8nLPPPpsPPviAhIQE8vPz2bNnD0D7FL+upv3tTyRyFwTB\n41m1ahWrVq1i+vTpzJgxg/3795Oens7kyZNZvXo1d9xxBxs2bCA0NNTtc27ZsoUFCxYQHR2Nj48P\nV155JevXr2fEiBFkZmbyi1/8gs8++4zBgwcDrqf97U8kchcEoX/pIcI+Xmitueuuu7jpppu6vLZt\n2zZWrFjB3XffzZlnnsk999xzTNcKDw9n586drFy5kqeeeoq33nqLF154weW0v/0p8hK5C4LgcXSe\n8vd//ud/eOGFF6itrQUgPz+f4uJiCgoKCAoK4qqrruL2229n27ZtLo93xZw5c/jyyy8pLS2lra2N\nZcuWMX/+fEpLS7HZbFx00UX8+c9/Ztu2bd1O+9ufSOQuCILH0XnK34ceeoh9+/Zx8sknAxAcHMyr\nr75KRkYGt99+O15eXvj6+vKf//wHgKVLl7Jo0SLi4uJYu3aty2sMHTqUBx54gIULF6K1ZsmSJVxw\nwQXs3LmT6667DpvNBsDf/va3bqf97U/cmvK3P5ApfwXBc5Epf/uGfp3yVxAEQRh4iLgLgiB4ICLu\ngiAIHoiIuyAI/cKJ6s/zFI71/om4C4LQ5wQEBFBWViYCf5RorSkrKyMgIOCozyGpkIIg9Dnx8fHk\n5eUhi/IcPQEBAcTHxx/18SLugiD0Ob6+viQnJ5/oYnyvEVtGEATBAxFxFwRB8EBE3AVBEDwQEXdB\nEAQPRMRdEATBAxFxFwRB8EBE3AVBEDwQEXdBEAQPRMRdEATBAxFxFwRB8EDcEnel1CKl1AGlVIZS\n6k4Xr/9aKbVXKbVLKbVGKTW874sqCIIguEuv4q6U8gaeABYDE4DLlVITOu22HZiltZ4CvAP8va8L\nKgiCILiPO5H7HCBDa52ptW4G3gAucN5Ba71Wa11vf7oJOPqpzARBEIRjxh1xHwbkOj3Ps2/rjuuB\nT4+lUIIgCMKx0adT/iqlrgJmAfO7eX0psBQgMTGxLy8tCIIgOOFO5J4PJDg9j7dv64BS6izg98D5\nWusmVyfSWj+jtZ6ltZ4VHR19NOUVBEEQ3MAdcd8CjFZKJSul/IDLgOXOOyilpgNPY4S9uO+LKQiC\nIBwJvYq71roVuBVYCewD3tJapyql7ldKnW/f7SEgGHhbKbVDKbW8m9MJgiAIxwG3PHet9QpgRadt\n9zj9f1Yfl0sQBEE4BmSEqiAIggci4i4IguCBiLgLgiB4ICLugiAIHoiIuyAIggci4i4IguCBiLgL\ngiB4ICLugiAIHoiIuyAIggci4i4IguCBiLgLgiB4ICLugiAIHoiIuyAIggci4i4IguCBiLgLgiB4\nICLugiAIHoiIuyAIggci4i4IguCBiLgLgiB4ICLugiAIHoiIuyAIggci4i4IguCBiLgLgiB4ICLu\ngiAIHohb4q6UWqSUOqCUylBK3eni9dOVUtuUUq1KqYv7vpiCIAjCkdCruCulvIEngMXABOBypdSE\nTrvlANcCr/d1AQVBEIQjx8eNfeYAGVrrTACl1BvABcBeawetdZb9NVs/lFEQBEE4QtyxZYYBuU7P\n8+zbBEEQhO8ox7VDVSm1VCmVopRKKSkpOZ6XFgRB+F7hjrjnAwlOz+Pt244YrfUzWutZWutZ0dHR\nR3MKQRAEwQ3cEfctwGilVLJSyg+4DFjev8USBEEQjoVexV1r3QrcCqwE9gFvaa1TlVL3K6XOB1BK\nzVZK5QGXAE8rpVL7s9CCIAhCz7iTLYPWegWwotO2e5z+34KxawRBEITvADJCVRAEwQMRcRcEQfBA\nRNwFQRA8EBF3QRAED2RAi3tdUytr9hX16Tm11vz27Z18lV7ap+cVBEE4ngxocX9vWx7Xv5RCQWVD\nn50zq6yed7bm8emewj47pyAIwvFmQIt7boUR9cKqxj4755ZD5QDku1FhHCqt49QHv+DljVlorfus\nDCeabTkVbM2uONHFEAThGBjQ4m5F7MXVfSfu32YZcc+r6F3cd+VVklfRwD0fpvLAZ/v7rAwnmvs/\n2sv/vbf7uF7z3uWp3PRKynG9piB4MgNa3A/bI/aiPhT3LXZxz69o6DUar6hrBmDC0MF8ecBzJkLL\nq6gnvbiGuqbW43bNLVnl7MitPG7XEwRPZ0CLu2XHFNc09cn5iqsbyS6rZ1hYIA0tbVTUt/S4f3ld\nM0rBtMSwPrWGTiSNLW2U1jZj07Anv+q4XTe3vJ7S2mbabJ5jbwnCiWTAinubTXO42orc+0bcLUvm\nwulmuvq8ivoe9y+rayY8yI9hYYFUNbRQ33z8It3+wrlzelfe8RH3qvoWqhtbabNpyu2tIcFzaGmz\nsbeg+kQX43vHgBX3kpqm9iivuKb7qFlrzcrUw25FhBvSSgn29+HsiTGAsWZ6oryumYhBfgwNDQAc\nNtFAxrkjeWfe8bFJcp0q0Z4+S2Fg8sH2fM59fAOFVX2X1Sb0zoAV9wL7FyXA16tHz/3bQ+Xc9MpW\n1h0obt9W3djS5ZjmVhufpR7mrPFDGB45COi9U7XMLu6xHiTuVuQ+JT70+Il7ubO4900rTPjukFFS\ni03DweK6Izru410FPLzywDFfv6m1jRe/PkRL2/drFdABJ+6ltU2s2F3YLqSTh4X2KAiZpeYLdajU\n8cX688d7ufyZTR32+/pgKVUNLZw7JY7QQF9C/H16TYesqGsmIsiPoaGBQPcpmZX1zQMmVTK/ogEv\nBWdPiCG3vIG3UnL53zd3MOvPq/stPTLHWdydKt3mVtuAuW8WeRX1zPjTag4crjnRRfnOkFdufkdZ\nZUcm7u9ty+eFrw8d83fgywMl3PfRXtaneU7SgzsMOHFftjmHn7+2jW12oZkaH0ZlfQuNLW0u988u\nM8LhHIXvzq8ms7SOmkZHh+nHOwsJCfDhtDFRAAwLD+zVcy+vayYi2I/YwfbI3UULoqSmiTl/XcOb\nW3K7vOYO9y5P5cl1GUd17NGQV9lAzOAA5o6IBOB37+zi831FVDW08Onu/hnYlVtRzyA/bwCK7f0n\nVQ0tzPjTaj7fV9zTod859uRXUV7XzK7j1OoZCFi2W/YRintueT31zW2UHWM/jBV0fd98/wEn7udN\njQPgjS25BPp6MyYmBDAi6oqc8jr7o/mCtdk0B0tqAUgrMo8NzW2s2nuYsyfE4u9jRCY+PLBHW8Zm\n01TUNxM5yI9AP2/CgnxdeopbsytobrXx32+yaGpt43fv7GSnmyl/Wmve25bHqtS+nWKhJ/IrGhgW\nFsis4eEsu/EkPvvVaaTcfRYzh4ezMbOsX66ZU95AcvQgQgN921thmSW11Da1un2vjpXWNlu3AcKR\nYH1nPMGi6yss2y2rrOdgyRmtdfu9dLbtjgbLwt1bKOL+nSYpahBT4kOpbWplaFgAMXa/uzvf3Yrc\nrS9Ibnk9za3Ge0srMk3nt7fmUtPYyo9nO5aKjQ8P6rFDtbKhBZuGiEF+AMQODnD5g7Z86/2Ha7ht\n2XbeSsljhZsRcHFNE9WNrW6Nlu2N3PJ6nv+q9yZuQVUDcWGBKKU4eWQk42IH4+/jzckjothbWE1l\nfd9ns+SV15MYEcSQEP/2DlXrPef20nrqKx5ZncbMP61m7f5iKuqau1TU6w4UuxV5WuUu7MOxF98F\n7vlwDze8dOSDzGqbWttTio8kci+tbabBXtnmujGgsCes36WI+wDgvCkmeo8LDSRmsD/guiNOa02O\nJe4V9Wit2wUd4MDhGlrabDz9ZSYzh4czOym8/bWkyCBqmlq7/UKW15nrWeI+NDTApS2zM7eSkdGD\nCPT1ZqU9As8sde9Lnm5vWZTUNB1zVPnqpmz+9PHeDv52Z9psmsLKRoaFB3Z57eSRkWgNmzLLj6kc\nmzPL+Nfn6fxn3UGaW23YbCZCSwgPYshg//bP0apY3Rkp3BsZxbVkFPfsga/YXUhdcxs/fWkL0/+0\nmrMfWd9+z202zc9e3cZja3q3x/I9MHLXWvPpnsNsPlR2xP63FVQNCwsku6wem5vjGJwr9WON3Asr\nzWeRXVbfwYq1+GJ/EU9/efCYrgHmPm3K7P4epWSV83bK0dmzR8OAFPdzpw4FjKDGhHQfuVfUt1DT\n1EpSZBCNLTZKaptILzaCOWpIMGlFNXy8q4D8ygZ+Nn8kSqn2Y8+eGAvAhzsK2rdVOXn75XXmSxI5\nyFQusaGBXX7QNptmV14Vp4yM4ofT4xgc4MOMxLD2zt1/rk5rH5V530epXSJ654qooLKB1IKqox5Y\ntKfAHOdqFGirPYuguKaRVptmWFhXcZ+aEEqArxeberFmmlrbeozQ7nh3F//8PI0HP9vPNwdLKapp\npLnNRkJEEENCAto99/bIvdMPW2vNjS+nsDL1cI/lcObW17fx0/+mdCssWaV1ZJXV87tFY7l14Sgu\nnD6MmqZWUu33rLC6kYaWtnY7rycsC+BIJrOrbmxp/ww6l6ustufsodSCKr49dGwVbm9kldVTUtNE\nTWMrlb0M7OuM9fmdOiqKplYbRW6mujp/7r31ffVGYXUD4UG+AOwr7FrJv/RNNv/8PM3tiqc7VqYW\ncdkzm7oNgP6z7iB/+HCPy8+6PxiQ4j40NJC/XjiZa05JIizIFz9vL5cDmSyRmTfKdJLmlteTUVxL\nXGgAMxLDSCuq4al1mYyJCeaMcUM6HBsXFsjc5Ag+2J6P1hqtNRc88RU/f20bWmuXkXtpbTNNrY4I\n+6DdN56aEMYfz5vImt8sYHZyBDll9RRWNfCvNek8/9UhimsaefHrLO56b3f7lAYA6U7RZn5lA3e+\nu5trX9xCQ3Mb69NKuOGlFBY9up7P9hSyr7Ca8x7/is0uxFdrTaq9M2l7Tkdxf3xNOtPuN5kwVqqa\nq8jd38ebWcMj+Dqj56mQX/w6izP+8aXLSqihuY3s8nquPSUJMFbVoRJzzeGRxpYpqWlCa90eARd3\narUUVDWyem+R2527eRX17D9cQ055fbd9BuvTTRbFOZOG8puzx/J/54wHaM8OyrJXxgdLanuNXNsj\ndzdtmeZWGwsfWsfT6zM7bNdac+Vzm7nvo71dr1HZ0D7Y6/fv7+F/39zh1rWOli1OlUdvGS9aa65+\nfjNP2SNhy1I5dbT5DWaVuifUVottbEwIueWOivJgSW2H31hv2Gyaw1WNLBxrft97C7p+LzOKa2ls\nsZFf2cBHOwu4+ZWtR9VS/tKejbM733U/UUaJuY4VYPY3A1LcAa6Ym8ikYaEopYgNDXAZVVkWxGmj\nLXFvIL24hlExIYyJCaG0tpkDRTXcPH8kXl6qy/EXTh9GZmkdu/OrSC2oJqusni/2F/PF/uL2Hvx2\nz93u/Rc7VTJWlDwtIYwAX2+iQ/wZETWI5jYbK3abyHPjwVI2HjSiU9XQwiOr09qPTyuqJd4utIdK\n69h/uJrS2ib+umIfN72ylT35VTS32bjl9e1c9swmdudX8d9vstqPf3Z9JoseXU92WX17xLXdKXJ/\nfE06/1idRlNrG//75g5+/8FuooL9mZ4Q5vKenzFuCOnFtV3u9VfppTyz3vyY16eV0GbT/OHDPV0i\nISOOMDspgqGhARw4XMMueyUwMS6U6BB/mttsVDW0kFfRgNWQcu5zsDIeDhT1/AP544d7eGjlftbu\nN9k2Ab5evNFNxtKXB0pIjAgiKcqMb4gO8Wd4ZFC7uFs2Wk1jKyX2SLqhuY13t+Z1eI/1zcZfDvH3\nobK+hYbm3gViV14lZXXN7eW0KKxqJL+yof07tCe/ir+u2Me5j29g3gNfcN1/t1BR18zOvEryKxs6\nBAVHi3N/inNO+OZD5Xjbfx/ZvXSK5lc2sCG9lAc+3c9/1h0kt9xkQk2zf6fc9d1zy+uJCvZjTGxI\n+++4oq6ZxY9u4PmvDrn9nsrqmmlp00xNCCNykF8X372uydGnlVFcy3vb8vgs9TD3f9y1Uu0JrXV7\nqqWrrJzGlrb21sjxyqQasOLuzMKx0axPK+nip1lfxJNH2KOGsjoyimsZPSSYsbEmy2ZYWGB7Bk5n\nFk8eip+3F8u+zWX13iK8FCRGBPGnj/dSZLdgwgeZ5p41SvWR1WnsLahGa83qvUWE+Pswwi4aAMlR\nwYAZtQem4+jljdmE+Ptw5dxEXtucTU6Z6R9IL6rhtNFR+Hgp1u4vpqVNE+zvwyubsvHz8eL9W05h\n+a2nMjspnEF+3pw1PoYv9hdT19TKvsJq/r5yP/sP17RHUXOTI9hXUN1unTy6Jp3zpsbx6vVzyauo\np7CykaevnklYkF8398NYVSt2OaLmJ9dlcPULm/nriv3syqskJbuC5KhBbM+p5N1teR2Ot1oiY2LM\n/d9XWM2uvEoSIgKJGOTHEHtKaXFNE/mVDUyMGwx09N332X+cB4tru23eNra08caWXJ5cd5CXNmYz\nPDKIy2YnsnLP4S4i2NjSxsbMMuaPie6wfUZiONtyKtFat7cuzHXN/69syuI3b+/k64OOlowVtU8f\nbvpu3IneLZtrZ15lh2jREvWc8nqq6lv4+Wvb+O/XWQT6evODCTHszK3kv99kYTUkUguqySiu4ZFV\nB7j7g93d+tSNLW3sP9xVfD7ckc/MP39ORnEt23IqmHjPyvZc/S1Z5Zw2Ogqleo/crRbi1PhQHvxs\nPyt2F5IQEURcWCB+3l4cclfcK+qJDw8iITyQgsoG2mzaZJ612fgmw/2sLatjfGhoABOHhbIzt2Pk\nnun02aYX17A7v5pBft68vjmHVUdg/WWV1ZNf2YC3l2q/B84cKq3DigN2HqdpPTxC3M+fFkdTq43P\nO63KlFVWR+zgAEKDfIkZ7M/yHQU0ttgYPSSYCUMH4+/jxS0LR+Hr7fo2hAb6cunseN5OyeWdrXnM\nHB7OvedPIKusnje25BLi79OeOjk9MZwlU4ayYnch5//7K5a+spVVe4u48fQRHVoFyXah351fxZAQ\n49dvza5gTnIEvzhjNEopXtuc3Z4pMy52MLGhAXxt/0Lff8FEQgN9eeBHkxkaGkiwvw/LbjyJdbcv\n5MbTkmlqtfHe9nx++/ZOQgN9CQvy5a2UXLwUXDYngeY2G6kF1TyxNgNvL8Ufloxn7ohIHr98Bs9d\nM4uZw8PpjqGhgcwcHs4ndkskJaucv392gLPGx+Dtpbh3eSrNrTbuXjKe0UOCeW9bfofj04pq8fFS\nJEUNYlzsYA6W1LItu5Ip8Saqs+5HRrGxs+Ymm1z73PJ6csvrO8xR0txm6za1bkduJU32jKiM4loW\njh3Cj2eb9/7e9o5l+nBHPvXNbSyeFNth+4zh4ZTUNJFX0UBWWR2R9haa1Wr5xF7Bfb7X8Z3Ls0eA\ns+z3sHPGzaHSuvZpMKxHKypuadNsy6loH7jl3DeyMvUwOeX1/N8543j75lP464WT8fZSPLkuo318\nwJ6CKu5dvpfH12bw6qYcXtmU3f7+rewwsFpzG9iQ7hjQY7Np/v1FBm02zef7ilixq5DmNhtrDxRz\nuKqRnPJ6Th0VRVxoYJfIvai6kfVpJe12VWp+FV4KXr5+LjOHh1Nc00R8eBDeXorJ8aEs25zjVr9R\nbnkDCRFBJEQE0WrTFFY1sCXb2ENbsys6tCx68sqtHHfLZj1QVNNh/iIr4PBSsCG9lNLaJn599lji\nQgN4Z2uey3O6wrqfSyYP5WBJbRdbJ8NuxUSH+EvkfiRMTwhnWFggr2/O4YaXUrjltW28uSWH1alF\njLFH6AnhQWSW1jFzuBHhyGB/Uu4+iyvmJvZ47tvOHI2vtxf5lQ38YEIMC8cOYVxsCMU1TUQEOyLc\nYH8fnrhiBpv/70wWjI1m9d4izp8axy/OGNXhfFHBfoT4+wBw1oQYhkcGASYbJTY0gLMnxPBmSm77\ndAmjY4IZFhZIc5uNQX7e/HDaMLbefRaLJw9tP6dSCj8fL2YnRRAz2J8/fLCHtKIaHrxoCudOGYpN\nw8joYE4ZaVow//4ig/e25XPFnMT2aHnJlKGc3il6dcWSyUPZf7iGjOIa/vl5GlHBfvzrsmmcOiqK\nbTmVeHsp5iRHcOb4GLZklXdoTaUX1ZAcNQhfby/GDw2hpc1M/jY1PhSg/V5Ywjk9MQw/by++zijl\njH+s47E16ewtrCbJvp9zh7MzmzLL8FLw27PHAma07fihg5maEMabW3LQWtPU2obNpnl2wyEmxg3m\n5JGRHc4xM9EI9NbsCg6V1jF3RARBft4cLKklt7yenXlV+HorPt9XjNbaNO8rOop7fkUDH+7Ip7qx\nhV15lZzxj3W88NUhKsyLDXYAABMlSURBVOubmffAFzyy6gApWRWcPzUOpeCzPYdZ+PA67v94L9tz\nKtoDgWc2GD/e6juKDvFn3qgoWto0C8YOIT48kK8zStmYWcbN80dy6qgo1u4vJr+ygUWPruePy1Pb\n39fndvvnN2/tbO+sXbO/mPTiWvy8vVh3oJi19u/epswyvrDvP29UFMMjg7pE7vd8uIefvPAtt7+z\ni8aWNlILqhkZHUxooC//uXIGw8ICmZZgPt9/XTaNkABfrn5+M6U9dBS32TQFlQ0khAeSEG4+69zy\nBrYcKsfHS9HQ0tZeQTyxNoOTH1jTbXZSob3CjQ0NYG5yBECHDuiMYhNwzEgMb+9Pmp4YxlkTYlif\nXuLSWluVepiZf1rdPpq6saWNj3cWkhARyDmTY7FpuoxQziiuxUvBuVOGsr+wpk/GVPSGW+KulFqk\nlDqglMpQSt3p4nV/pdSb9tc3K6WS+rqgPeHlpTh36lC2ZFXwdUYp6w4Uc8e7u0mMDOKvF04C4Ken\nJnPbGaNYduNJhAQYK8V67IkhIQHccFoyXgp+MCEWpRQ3njYCcPjtzoQF+fHM1bN4Y+lJPHzJ1A4Z\nOGCEODna/GinxYdxil1ULHG5+uThVNa3cMe7uxk1JJhpCWHE27/gE+IG4+Wl8OmmpeHlpbh8TiJR\nwf68ev1czhwfww+nmRkuJw0LJWaw6Uj+Yn8xwQE+3Dx/ZK/vvzPnTB6Kv48XFz+1ka8zjJgE+flw\nwTRjbU0eFkpIgC8Lx0bTatMd1qJNK6ptH3Rm2WJAe+Q+NDSQk0ZEtLcMEsKDGBYeyKd7DtPSpnl5\nYzY55fWcNzUOL2Usmltf38Zd7+1mVephfvz0Rv65Oo1NmWVMjAvl5wtG8vEvTuUUuyhePjuBtKJa\nHvhsP5P+uJIlj39FRnEtS08f0eVzGhsbQliQLx/tLCCnvJ7kqEGMjA4mo7i2vXw/mz+S/MoG7npv\nN1PuW8Wrm7Lx8VJMsfvLL36dxS/f2MFv39rJw6vS0Bpe/zaHt1PyOFzdyGNfZNDQ0mYqn9jBvLwx\nm/zKBl7emM3O3CoWjI0mLjSAjOJaokP8GTUkuL18F0439/v0MVFMjBvMhvRS2myaRRNjWTA2mvTi\nWv65Oo1Wm+aNLTnG269tYldeJUsmD6WyvoX7Ptprj9rTiQ8P5JpThvPtoXIOltQR7O/DlkPlfLK7\ngKTIIMbFhjA8Mqg9tRhM9tja/SWMHhLMu9vyeGR1GqkF1UwaZsR8yOAAvrx9AbeeMRowY0eeu2YW\nFfUtvJ2SR1NrGx/tLKC+uZX65lbeTsmlqqGFnXmVtNo0CRFBJEaY7/7mQ2Xszq/iAvv3+dtD5aw7\nUMzDqw5QVN3Eg90sllNY3YiftxcRQX5MiQ9rz/hqam2jtqmV9OJakqMGMW5oCDYN3l6KCUMHc/aE\nWBpbbHzlIoHguQ2HKKtr5q2UXIprGrnoP9/wbVY5P52XzMQ48947WzMZJbUkRAQxJymCVptutxf7\nE5/edlBKeQNPAD8A8oAtSqnlWmvnHofrgQqt9Sil1GXAg8CP+6PA3XHdKcnUNbVy/akjCAnwYe3+\nYs6dEkegvdl6zuShnOMU7R4JvzprDOdNjWuPpM6bGsc/Vh0gzkXKIBiRPWlEpMvXwFgzu/KqmJoQ\nxqRhoQT5+TA+1vjLJ4+IZG5yBCEBPvzj0mkE+fm0Z69YX5ye+OWZo7ntjNHtVtDM4eH8eFZCe7/C\nez+f196E7ixo7hAbGsB7Pz+Fu97bTbB/M1fOHQ6Y1NHBAantWUczh4cTEuDDZ6mH2XyonGFhgeRW\n1POjGebHOSIqGF9vRatNt4sBwGWzE9tTyYaFBxIfHsih0jqmJYS1WxXTE8NIihrEyxuzqWpowUvB\nsm9zCPD1arc5fjovCaVUh3OfOzWO+z/ey9NfZjIxbjAlNU0kRw1y+b3w9lJcNXc4/15rctuTo4LJ\nr2jgq4xSssvqmZoQxtUnJ/H42ox2i27/4RoSIoxVFhbky97Cavx8vFhlt26mJ4axPaeSRz9PY2p8\nKDVNrWSW1DEnOYK5IyLYW1jN0tNH8OqmbOqb25iWEEZeRQMFVY2cMjKyw+d17pQ46prauGDaMIqr\nm1iZWsTQ0ACmxIcSHODDnz/Zxztb85g1PJyssnru/mAPV80djtZw0/wRjIgexONfZODv48XOvCr+\ncclUhoUH8uwG01l58/wRPLwqja8zyrhpvqn8hkcOoqyumerGFgJ9vfl0j7Fv/nHpVJ7bcIiXN2bR\n2GJr7ysBugQi44cOZk5SBG9uyaG6sYX/rDvIqCHBKCC9uJZ/r82grqmV2MEBnDl+CNHB/pw2Oop/\nrUlHa1g8KZbtORV2YW1ibEwI80ZF8fxXh7hybiKzkiI6XK+wspHY0AC8vBR+XoqZw8P5KqOUi/7z\nDWW1zWhtPpdR0abiHD0kmABfb+aOML/BVamH+cGEmPbzZRTX8m1WOb7eimXf5rIjt4qM4lqe+8ks\nzpoQg9aakAAf9haalkVFXTONrW0cLK5lVHRwe8W/K6+K6YndW6B9Qa/iDswBMrTWmQBKqTeACwBn\ncb8AuNf+/zvAv5VSSh/HWZ9iQwP48w8ntz+/ZFZCD3sfGd5eqj3iBPDz8eKdn52Cv8/RuVpzkiPY\nkVvJqCHBJlKIm9D+mlKKN286ucP+VsbMBKcfTXcopXDWbKUUD148pcs+x8LEuFA+vGUeLW0aP/s9\nCPb34cvbFxIcYL5SPt5enD4musM4AaD9Pvr5eDEyOphWm+kktlg0KZbQ5b40tbYROciPpMhBbMos\n48krZ/DT/25h/+EaJgwNZWxMCJ+WHGbysFCevHIGO/MqOW10NFc8u4nUgmqXlWuwvw+3LBzFgcM1\nPHDRZHy8vGiz6W77XK45JYlnNmTS3GojOWoQhZX/3965B1ddXHH8c5Lc3JAHeUNCHiQhgQiEkRAM\nKCIWFYKKAlZBrDhqaas4WPsYHKaWdkat2tqXitMKU2spOkx9hI4M2opap0UMkPCQtzwMJkQeAgqB\nPE7/+G3u3MTcPJT8bnLdz8ydu3ezye+bs3vP7p7d32/P8mrlJ8R5m/nVzCJS47yUjUyjoUl5ZEYR\nc59bT3aSMwBI6x/FZ2ca+Pn1w1mztZb9R79g+byxTHxiHafrG7lzQq6vw0qO9XL7+BySYyL5waR8\nojzhPPXWHsYMTuTA0TO8+eERLjMhtRY84WHcNs7pWFs6sGuGD0REyEuJITspmkPHz3DHZTmEiXDP\nik38rHYbKbGRjBwUT/6AWFZVVLNqYzWXF6QwsziDxmYlzhtBcmwkN4/N4tdvODu3ykY6nV9LOKzs\nd//hXGMTcVEe8lJiKMqI53tX5FFe5dR1Z+30lrFZ/GhVFUvf3selQ5LNY0CUX0wfwTNv70UV/nZ3\nKQPMPSyPzRrFlN++y+lzjYwZnMgluUm8+MHHjMqMZ+ltY0jo52HN1hrmv7CRpXOLaWpWTtU3cups\nAx8cOE6WGf0DlOYm+3akeSPCONforMEVmHY5yoQIPeFhXDlsAK9VfsKO2lMUZTiz7Hd3f0pEmLB4\n2kUsWf2hM3MrK+Qq0wGICBdnJbC6qoaxOUk8+vpOPjt7nsYmZeJQZya2bF6Jb/dQT9IV554B+O8h\nqwZKA5VR1UYROQkkAx1viu7DBBq1d4W5pYN9I96uUJydQHp8FOM7mA24jRPnb91JJLYJU80cncF7\ne47y8IyRrP/oGKsqqn1fHoCHrh8Obbr/KE848yfmsaPmFCLCfZPzmTUmk0EJ/VhUVsjqqhoG9vdS\nmNafNdtqWVRW6Ft4A3hmbjHL39vvi0+35d4r89vNb4/UOC83jclk5YZD5KXEkBjtYXfd5/zwqgLy\nzEjv6VuLfZ1l+YIJvo41Kymao5+fY1ZxJrPHZnO2oYlYbwS3XpLNP7fUMHWk8xyjlsdL56bE+MIX\nCycXcN2odDITo7myMJWXN1czaVjg9ZCSnEQuy0/mVtOmRIRpRem8uvkwVw8fiDcinJ9MGcYTa3dx\nbUEqYWFCdGQES6aP4PG1O3lkRhEigidcWHztRcT38zAgLoohqTGcPd/kWxPJH+A4wGZVBiX0Y0v1\nSR64eigiwohB8VwxNJV3dn/KiPSOZ5jTitJZsno7YSL8cc5ovJ5wM+L1MLM4g4YmbRXyHJTQj9/P\nuZiNB0+QGBPJPZPyKUyLY05ptm9Dw4rvjuO2597nljZPe81I6Mdcv3W1CQUpPPnmbhZOLiB/QCz3\nrdzMyIx4CtPiiIwI8y3iA9z3rXyiI8N9+99XbjgEwNQRacwpzeapdXsZ2D+KuybktrrmwzcWMXfZ\neha+WElKrJdRmQls2H+cwrQ4RITJFw3EDaSzwbWI3ARMVdW7zefvAKWqusCvzDZTptp83mfKHG3z\nt+YD8wGys7PHHDx48EL+L5ZeiKr6nN/Z802+MNnX5cQX56k4eKLVlLkn+PxcI1uqP/MtRneV6hNn\nqG9o8jnEFpqalYamZqI8F8YOgWhsauZcYzMxZlakqpRXfUJpbrLvnozO2LD/OE3N2mqxeePBExSm\nxRHlCWf9R8coyUn0OdiPj59h06ETvrh4R6zbVUe0J9z39NELQd2petZsqyU3JYbk2Egiw8OckE+b\nmereutMMSXXya0/WM7C/FxGh7nQ9KTHedu95aWhqZlftaWpO1lOcnUByrJeDx74gLsrT7tpb7cl6\nnn1nH3dcmkNWUjT/3XeU8XnJAdfLuoOIbFTVkk7LdcG5jweWqOoU8/lBAFV91K/MWlPmfyISAdQC\nqR2FZUpKSrSiwp52b7FYLN2hq869K93IB0CBiOSKSCQwGyhvU6YcmGfSNwFvuRlvt1gsFktrOo25\nmxj6AmAtEA4sV9XtIvJLoEJVy4FlwAsishc4jtMBWCwWiyVIdGVBFVV9HXi9Td5Dful64NsXVprF\nYrFYviohcYeqxWKxWFpjnbvFYrGEINa5WywWSwhinbvFYrGEINa5WywWSwjS6U1MPXZhkU+Br3qL\nagq999EGvVWb1dU9rK7u01u1hZquwara6fO5g+bcvw4iUtGVO7SCQW/VZnV1D6ur+/RWbd9UXTYs\nY7FYLCGIde4Wi8USgvRV5/6nYAvogN6qzerqHlZX9+mt2r6RuvpkzN1isVgsHdNXR+4Wi8Vi6YA+\n59w7O6zbRR1ZIrJORD4Uke0istDkLxGRwyJSaV7TgqDtgIhsNdevMHlJIvKmiOwx7z17gOOXNQ3z\ns0mliJwSkfuDZS8RWS4ideagmZa8dm0kDn8wbW6LiBS7rOsJEdlprv2KiCSY/BwROetnu2dd1hWw\n7kTkQWOvXSIypad0daDtJT9dB0Sk0uS7YrMO/IN7bUxV+8wL55HD+4A8IBKoAoYHSUs6UGzSccBu\nYDjOWbI/DrKdDgApbfIeBxaZ9CLgsSDXYy0wOFj2AiYCxcC2zmwETAPWAAKMA953Wdc1QIRJP+an\nK8e/XBDs1W7dme9BFeAFcs13NtxNbW1+/hvgITdt1oF/cK2N9bWRu++wblU9D7Qc1u06qlqjqptM\n+jSwA+cs2d7KDcDzJv08cGMQtUwG9qlq0M5ZVNV3cc4e8CeQjW4A/qoO64EEEUl3S5eqvqGqjebj\neiCzJ67dXV0dcAPwoqqeU9X9wF6c767r2kREgJuBlT11/QCaAvkH19pYX3Pu7R3WHXSHKiI5wGjg\nfZO1wEytlrsd/jAo8IaIbBTn3FqAgapaY9K1gDun9LbPbFp/2YJtrxYC2ag3tbs7cUZ4LeSKyGYR\neUdELg+CnvbqrjfZ63LgiKru8ctz1WZt/INrbayvOfdeh4jEAv8A7lfVU8BSYAhwMVCDMyV0mwmq\nWgyUAfeKyET/H6ozDwzKNilxjmqcDqwyWb3BXl8imDYKhIgsBhqBFSarBshW1dHAA8DfRaS/i5J6\nZd21YQ6tBxKu2qwd/+Cjp9tYX3Puh4Esv8+ZJi8oiIgHp+JWqOrLAKp6RFWbVLUZ+DM9OB0NhKoe\nNu91wCtGw5GWaZ55r3Nbl6EM2KSqR4zGoNvLj0A2Cnq7E5E7gOuAucYpYMIex0x6I05se6hbmjqo\nu6DbC0BEIoCZwEsteW7arD3/gIttrK85964c1u0KJpa3DNihqk/65fvHyWYA29r+bg/rihGRuJY0\nzmLcNlofYj4PeM1NXX60GkkF215tCGSjcuB2s6NhHHDSb2rd44jIVOCnwHRVPeOXnyoi4SadBxQA\nH7moK1DdlQOzRcQrIrlG1wa3dPlxFbBTVatbMtyyWSD/gJttrKdXjS/0C2dVeTdOj7s4iDom4Eyp\ntgCV5jUNeAHYavLLgXSXdeXh7FSoAra32AhIBv4N7AH+BSQFwWYxwDEg3i8vKPbC6WBqgAac+OZd\ngWyEs4PhadPmtgIlLuvaixOPbWlnz5qys0wdVwKbgOtd1hWw7oDFxl67gDK369Lk/wX4fpuyrtis\nA//gWhuzd6haLBZLCNLXwjIWi8Vi6QLWuVssFksIYp27xWKxhCDWuVssFksIYp27xWKxhCDWuVss\nFksIYp27xWKxhCDWuVssFksI8n+krajhnaUQiQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjD9SGrvhzc",
        "colab_type": "code",
        "outputId": "ffff7660-21e1-4d66-baaf-5cbb1cb1e108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "my_classifier.eval()\n",
        "\n",
        "num_test_samples = len(test_dataset)\n",
        "random_idx = random.randint(0, num_test_samples)\n",
        "\n",
        "test_input, test_label = test_dataset.__getitem__(random_idx)\n",
        "test_prediction = F.softmax(my_classifier(test_input.unsqueeze(0).to(device)), dim=1).argmax().item()\n",
        "print('label : %s' % classes[test_label])\n",
        "print('prediction : %s' % classes[test_prediction])\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(test_input))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "label : car\n",
            "prediction : car\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwcY8z_C2QLA",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2W0ZtF82Xyb",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\"> Fill here with your discussion </font>"
      ]
    }
  ]
}