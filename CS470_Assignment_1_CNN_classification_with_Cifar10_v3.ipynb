{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470 Assignment #1: CNN classification with Cifar10_v3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmoon/CS470/blob/master/CS470_Assignment_1_CNN_classification_with_Cifar10_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqPMAzzNipD",
        "colab_type": "text"
      },
      "source": [
        "CS470 Assignment #1: CNN classification with Cifar10\n",
        "====\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1mgGV_uOIK",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Connect to your Google Drive\n",
        "\n",
        "It is required if you want to save checkpoints and load them later on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLth6ZfXuSGT",
        "colab_type": "code",
        "outputId": "e664865e-18f5-42fa-a5e3-0851c87e4467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwUwGf8qW1U",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtshANjqpy4",
        "colab_type": "code",
        "outputId": "b5e87f56-ffff-4ded-cf17-3778109f245a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJ-Q6sbq8c3",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Configure the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5jAy7Wq-E2",
        "colab_type": "code",
        "outputId": "28ed7fbd-6bce-411c-c32c-36e06cc5245d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 200\n",
        "learning_rate = 0.001\n",
        "batch_size = 20000\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://343138bc.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tZt60aMrQ1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Construct data pipeline\n",
        "\n",
        "**`torchvision.datasets.CIFAR10`** will automatically construct **`Cifar10`** dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbtV46LrXOF",
        "colab_type": "code",
        "outputId": "c1ffcd8d-637b-438f-fdbc-bea399882823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_dir = os.path.join(gdrive_root, 'my_data')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_dWd-6rwWb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Construct a neural network builder\n",
        "\n",
        "We serve the baseline CNN model which is supported on Pytorch tutorial: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb#scrollTo=c1E1b7-igUcR\n",
        "\n",
        "### (You have to compare your own CNN model's test accuracy with the baseline CNN model and explain why your own model's test accuracy is higher than the basline.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_wne0Vr1E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
        "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat([out,x], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_planes)\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(F.relu(self.bn(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyDenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
        "        super(MyDenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        num_planes = 2*growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans2 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans3 = Transition(num_planes, out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes, num_classes)\n",
        "\n",
        "    def _make_dense_layers(self, block, in_planes, nblock):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers.append(block(in_planes, self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
        "\n",
        "def DenseNet169():\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
        "\n",
        "def DenseNet201():\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
        "\n",
        "def DenseNet161():\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
        "\n",
        "def densenet_cifar():\n",
        "    return MyDenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpA3xhjMspvA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Initialize the network and optimizer\n",
        "\n",
        "If you want to train modularized neural network in Step 5B, please use 'MyClassifier2' as 'my_classifier'. It is written as a comment now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP111gW0s8aH",
        "colab_type": "code",
        "outputId": "6792f031-fd97-4ab5-87fa-865b713bac5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = 'cuda'\n",
        "my_classifier = densenet_cifar()\n",
        "my_classifier = my_classifier.to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        " \n",
        "\n",
        "# Print your neural network structure\n",
        "# print(my_classifier)\n",
        "print('parameter size : ', count_parameters(my_classifier))\n",
        "\n",
        "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter size :  1000618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAQeXmjsILS",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Load pre-trained weights if exist\n",
        "\n",
        "- **For your sumbmission you have to store the trained model as a checkpoint.**\n",
        "- Please do not erase this step.\n",
        "- If you want to modify this step, please be careful.\n",
        "- After training please confirm that your checkpoint is correctly stored and re-loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLNZxaBsHUl",
        "colab_type": "code",
        "outputId": "12c1a111-90c9-4a49-cf5b-651378109ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best accuracy : 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHGB0Bcz5aA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "21e1e875-d5f8-42d5-fcba-bdb31bf59ab8"
      },
      "source": [
        "  !nvidia-smi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 22 11:52:30 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    74W / 149W |   6305MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1t7n6yttNEc",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train the network\n",
        "\n",
        "Note : It would be better to save checkpoints periodically, otherwise you'll lose everything you've trained if the session is recycled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vczdKbytV38",
        "colab_type": "code",
        "outputId": "667add5d-7d2a-4274-f585-99dbaf8a830d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if training_process:\n",
        "  it = 0\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(max_epoch):\n",
        "    # train phase\n",
        "    my_classifier.train()\n",
        "    for inputs, labels in train_dataloader:\n",
        "      it += 1\n",
        "\n",
        "      # load data to the GPU.\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # feed data into the network and get outputs.\n",
        "      logits = my_classifier(inputs)\n",
        "\n",
        "      # calculate loss\n",
        "      # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
        "      loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "      # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "      #       Otherwise, gradients will accumulate.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backprogate loss.\n",
        "      loss.backward()\n",
        "\n",
        "      # update the weights in the network.\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate accuracy.\n",
        "      acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "      if it % 200 == 0:\n",
        "        tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
        "        tbc.save_value('Accuracy', 'train_acc', it, acc.item())\n",
        "        print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
        "\n",
        "    # save losses in a list so that we can visualize them later.\n",
        "    train_losses.append(loss)  \n",
        "\n",
        "    # test phase\n",
        "    n = 0.\n",
        "    test_loss = 0.\n",
        "    test_acc = 0.\n",
        "    my_classifier.eval()\n",
        "    for test_inputs, test_labels in test_dataloader:\n",
        "      test_inputs = test_inputs.to(device)\n",
        "      test_labels = test_labels.to(device)\n",
        "\n",
        "      logits = my_classifier(test_inputs)\n",
        "      test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "      test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "      n += test_inputs.size(0)\n",
        "\n",
        "    test_loss /= n\n",
        "    test_acc /= n\n",
        "    test_losses.append(test_loss)\n",
        "    tbc.save_value('Loss', 'test_loss', it, test_loss)\n",
        "    tbc.save_value('Accuracy', 'test_acc', it, test_acc)\n",
        "    print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss, test_acc)) \n",
        "\n",
        "    tbc.flush_line('train_loss')\n",
        "    tbc.flush_line('test_loss')\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'my_classifier':my_classifier.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_acc':best_acc}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    \n",
        "tbc.close()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch:0, iteration:200] train loss : 0.4908 train accuracy : 0.8359\n",
            "[epoch:0, iteration:391] test_loss : 0.5652 test accuracy : 0.8038\n",
            "[epoch:1, iteration:400] train loss : 0.2680 train accuracy : 0.9141\n",
            "[epoch:1, iteration:600] train loss : 0.2882 train accuracy : 0.8828\n",
            "[epoch:1, iteration:782] test_loss : 0.5470 test accuracy : 0.8176\n",
            "checkpoint is saved !\n",
            "[epoch:2, iteration:800] train loss : 0.2735 train accuracy : 0.9062\n",
            "[epoch:2, iteration:1000] train loss : 0.3578 train accuracy : 0.8516\n",
            "[epoch:2, iteration:1173] test_loss : 0.5364 test accuracy : 0.8279\n",
            "checkpoint is saved !\n",
            "[epoch:3, iteration:1200] train loss : 0.2408 train accuracy : 0.9219\n",
            "[epoch:3, iteration:1400] train loss : 0.3431 train accuracy : 0.8984\n",
            "[epoch:3, iteration:1564] test_loss : 0.4774 test accuracy : 0.8425\n",
            "checkpoint is saved !\n",
            "[epoch:4, iteration:1600] train loss : 0.1404 train accuracy : 0.9531\n",
            "[epoch:4, iteration:1800] train loss : 0.1694 train accuracy : 0.9297\n",
            "[epoch:4, iteration:1955] test_loss : 0.4926 test accuracy : 0.8446\n",
            "checkpoint is saved !\n",
            "[epoch:5, iteration:2000] train loss : 0.1104 train accuracy : 0.9609\n",
            "[epoch:5, iteration:2200] train loss : 0.1622 train accuracy : 0.9297\n",
            "[epoch:5, iteration:2346] test_loss : 0.5389 test accuracy : 0.8384\n",
            "[epoch:6, iteration:2400] train loss : 0.1574 train accuracy : 0.9531\n",
            "[epoch:6, iteration:2600] train loss : 0.1993 train accuracy : 0.9297\n",
            "[epoch:6, iteration:2737] test_loss : 0.4964 test accuracy : 0.8542\n",
            "checkpoint is saved !\n",
            "[epoch:7, iteration:2800] train loss : 0.1023 train accuracy : 0.9766\n",
            "[epoch:7, iteration:3000] train loss : 0.0956 train accuracy : 0.9766\n",
            "[epoch:7, iteration:3128] test_loss : 0.5288 test accuracy : 0.8536\n",
            "[epoch:8, iteration:3200] train loss : 0.0898 train accuracy : 0.9609\n",
            "[epoch:8, iteration:3400] train loss : 0.0793 train accuracy : 0.9844\n",
            "[epoch:8, iteration:3519] test_loss : 0.5810 test accuracy : 0.8410\n",
            "[epoch:9, iteration:3600] train loss : 0.0628 train accuracy : 0.9844\n",
            "[epoch:9, iteration:3800] train loss : 0.0696 train accuracy : 0.9688\n",
            "[epoch:9, iteration:3910] test_loss : 0.5690 test accuracy : 0.8507\n",
            "[epoch:10, iteration:4000] train loss : 0.0437 train accuracy : 0.9844\n",
            "[epoch:10, iteration:4200] train loss : 0.1308 train accuracy : 0.9453\n",
            "[epoch:10, iteration:4301] test_loss : 0.5871 test accuracy : 0.8510\n",
            "[epoch:11, iteration:4400] train loss : 0.0827 train accuracy : 0.9766\n",
            "[epoch:11, iteration:4600] train loss : 0.1576 train accuracy : 0.9688\n",
            "[epoch:11, iteration:4692] test_loss : 0.5900 test accuracy : 0.8532\n",
            "[epoch:12, iteration:4800] train loss : 0.0910 train accuracy : 0.9609\n",
            "[epoch:12, iteration:5000] train loss : 0.1351 train accuracy : 0.9766\n",
            "[epoch:12, iteration:5083] test_loss : 0.5491 test accuracy : 0.8644\n",
            "checkpoint is saved !\n",
            "[epoch:13, iteration:5200] train loss : 0.0397 train accuracy : 0.9844\n",
            "[epoch:13, iteration:5400] train loss : 0.0685 train accuracy : 0.9766\n",
            "[epoch:13, iteration:5474] test_loss : 0.5610 test accuracy : 0.8596\n",
            "[epoch:14, iteration:5600] train loss : 0.0491 train accuracy : 0.9922\n",
            "[epoch:14, iteration:5800] train loss : 0.0435 train accuracy : 0.9922\n",
            "[epoch:14, iteration:5865] test_loss : 0.6597 test accuracy : 0.8404\n",
            "[epoch:15, iteration:6000] train loss : 0.0491 train accuracy : 0.9766\n",
            "[epoch:15, iteration:6200] train loss : 0.0773 train accuracy : 0.9688\n",
            "[epoch:15, iteration:6256] test_loss : 0.6064 test accuracy : 0.8573\n",
            "[epoch:16, iteration:6400] train loss : 0.0327 train accuracy : 0.9844\n",
            "[epoch:16, iteration:6600] train loss : 0.0610 train accuracy : 0.9844\n",
            "[epoch:16, iteration:6647] test_loss : 0.6350 test accuracy : 0.8582\n",
            "[epoch:17, iteration:6800] train loss : 0.0266 train accuracy : 0.9922\n",
            "[epoch:17, iteration:7000] train loss : 0.2444 train accuracy : 0.9531\n",
            "[epoch:17, iteration:7038] test_loss : 0.6258 test accuracy : 0.8596\n",
            "[epoch:18, iteration:7200] train loss : 0.0895 train accuracy : 0.9531\n",
            "[epoch:18, iteration:7400] train loss : 0.0333 train accuracy : 0.9922\n",
            "[epoch:18, iteration:7429] test_loss : 0.6373 test accuracy : 0.8600\n",
            "[epoch:19, iteration:7600] train loss : 0.0379 train accuracy : 0.9766\n",
            "[epoch:19, iteration:7800] train loss : 0.0151 train accuracy : 0.9922\n",
            "[epoch:19, iteration:7820] test_loss : 0.6113 test accuracy : 0.8653\n",
            "checkpoint is saved !\n",
            "[epoch:20, iteration:8000] train loss : 0.0312 train accuracy : 0.9844\n",
            "[epoch:20, iteration:8200] train loss : 0.0781 train accuracy : 0.9688\n",
            "[epoch:20, iteration:8211] test_loss : 0.6838 test accuracy : 0.8611\n",
            "[epoch:21, iteration:8400] train loss : 0.0059 train accuracy : 1.0000\n",
            "[epoch:21, iteration:8600] train loss : 0.0704 train accuracy : 0.9766\n",
            "[epoch:21, iteration:8602] test_loss : 0.7359 test accuracy : 0.8460\n",
            "[epoch:22, iteration:8800] train loss : 0.0354 train accuracy : 0.9922\n",
            "[epoch:22, iteration:8993] test_loss : 0.6134 test accuracy : 0.8696\n",
            "checkpoint is saved !\n",
            "[epoch:23, iteration:9000] train loss : 0.0118 train accuracy : 1.0000\n",
            "[epoch:23, iteration:9200] train loss : 0.0148 train accuracy : 1.0000\n",
            "[epoch:23, iteration:9384] test_loss : 0.6278 test accuracy : 0.8641\n",
            "[epoch:24, iteration:9400] train loss : 0.0282 train accuracy : 0.9922\n",
            "[epoch:24, iteration:9600] train loss : 0.0346 train accuracy : 0.9766\n",
            "[epoch:24, iteration:9775] test_loss : 0.7027 test accuracy : 0.8517\n",
            "[epoch:25, iteration:9800] train loss : 0.0228 train accuracy : 1.0000\n",
            "[epoch:25, iteration:10000] train loss : 0.0516 train accuracy : 0.9844\n",
            "[epoch:25, iteration:10166] test_loss : 0.6165 test accuracy : 0.8702\n",
            "checkpoint is saved !\n",
            "[epoch:26, iteration:10200] train loss : 0.0091 train accuracy : 1.0000\n",
            "[epoch:26, iteration:10400] train loss : 0.0162 train accuracy : 0.9922\n",
            "[epoch:26, iteration:10557] test_loss : 0.7601 test accuracy : 0.8519\n",
            "[epoch:27, iteration:10600] train loss : 0.0330 train accuracy : 0.9922\n",
            "[epoch:27, iteration:10800] train loss : 0.0316 train accuracy : 0.9922\n",
            "[epoch:27, iteration:10948] test_loss : 0.7098 test accuracy : 0.8601\n",
            "[epoch:28, iteration:11000] train loss : 0.0407 train accuracy : 0.9844\n",
            "[epoch:28, iteration:11200] train loss : 0.0807 train accuracy : 0.9609\n",
            "[epoch:28, iteration:11339] test_loss : 0.6796 test accuracy : 0.8623\n",
            "[epoch:29, iteration:11400] train loss : 0.0313 train accuracy : 0.9922\n",
            "[epoch:29, iteration:11600] train loss : 0.0440 train accuracy : 0.9922\n",
            "[epoch:29, iteration:11730] test_loss : 0.6453 test accuracy : 0.8662\n",
            "[epoch:30, iteration:11800] train loss : 0.0310 train accuracy : 0.9844\n",
            "[epoch:30, iteration:12000] train loss : 0.0296 train accuracy : 0.9922\n",
            "[epoch:30, iteration:12121] test_loss : 0.6749 test accuracy : 0.8635\n",
            "[epoch:31, iteration:12200] train loss : 0.0100 train accuracy : 1.0000\n",
            "[epoch:31, iteration:12400] train loss : 0.0276 train accuracy : 0.9766\n",
            "[epoch:31, iteration:12512] test_loss : 0.6495 test accuracy : 0.8726\n",
            "checkpoint is saved !\n",
            "[epoch:32, iteration:12600] train loss : 0.0102 train accuracy : 1.0000\n",
            "[epoch:32, iteration:12800] train loss : 0.0311 train accuracy : 0.9766\n",
            "[epoch:32, iteration:12903] test_loss : 0.6655 test accuracy : 0.8662\n",
            "[epoch:33, iteration:13000] train loss : 0.0383 train accuracy : 0.9844\n",
            "[epoch:33, iteration:13200] train loss : 0.0650 train accuracy : 0.9688\n",
            "[epoch:33, iteration:13294] test_loss : 0.6838 test accuracy : 0.8660\n",
            "[epoch:34, iteration:13400] train loss : 0.0090 train accuracy : 1.0000\n",
            "[epoch:34, iteration:13600] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:34, iteration:13685] test_loss : 0.6906 test accuracy : 0.8659\n",
            "[epoch:35, iteration:13800] train loss : 0.0052 train accuracy : 1.0000\n",
            "[epoch:35, iteration:14000] train loss : 0.0479 train accuracy : 0.9766\n",
            "[epoch:35, iteration:14076] test_loss : 0.6993 test accuracy : 0.8637\n",
            "[epoch:36, iteration:14200] train loss : 0.0270 train accuracy : 0.9922\n",
            "[epoch:36, iteration:14400] train loss : 0.0040 train accuracy : 1.0000\n",
            "[epoch:36, iteration:14467] test_loss : 0.6800 test accuracy : 0.8641\n",
            "[epoch:37, iteration:14600] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:37, iteration:14800] train loss : 0.0107 train accuracy : 1.0000\n",
            "[epoch:37, iteration:14858] test_loss : 0.6492 test accuracy : 0.8753\n",
            "checkpoint is saved !\n",
            "[epoch:38, iteration:15000] train loss : 0.0090 train accuracy : 1.0000\n",
            "[epoch:38, iteration:15200] train loss : 0.0059 train accuracy : 1.0000\n",
            "[epoch:38, iteration:15249] test_loss : 0.6597 test accuracy : 0.8676\n",
            "[epoch:39, iteration:15400] train loss : 0.0050 train accuracy : 1.0000\n",
            "[epoch:39, iteration:15600] train loss : 0.0578 train accuracy : 0.9922\n",
            "[epoch:39, iteration:15640] test_loss : 0.7672 test accuracy : 0.8605\n",
            "[epoch:40, iteration:15800] train loss : 0.0548 train accuracy : 0.9688\n",
            "[epoch:40, iteration:16000] train loss : 0.0240 train accuracy : 0.9922\n",
            "[epoch:40, iteration:16031] test_loss : 0.6674 test accuracy : 0.8753\n",
            "[epoch:41, iteration:16200] train loss : 0.0173 train accuracy : 0.9922\n",
            "[epoch:41, iteration:16400] train loss : 0.0287 train accuracy : 0.9922\n",
            "[epoch:41, iteration:16422] test_loss : 0.7358 test accuracy : 0.8659\n",
            "[epoch:42, iteration:16600] train loss : 0.0470 train accuracy : 0.9844\n",
            "[epoch:42, iteration:16800] train loss : 0.0178 train accuracy : 1.0000\n",
            "[epoch:42, iteration:16813] test_loss : 0.6744 test accuracy : 0.8733\n",
            "[epoch:43, iteration:17000] train loss : 0.0061 train accuracy : 1.0000\n",
            "[epoch:43, iteration:17200] train loss : 0.0668 train accuracy : 0.9688\n",
            "[epoch:43, iteration:17204] test_loss : 0.6802 test accuracy : 0.8695\n",
            "[epoch:44, iteration:17400] train loss : 0.0229 train accuracy : 0.9922\n",
            "[epoch:44, iteration:17595] test_loss : 0.6999 test accuracy : 0.8713\n",
            "[epoch:45, iteration:17600] train loss : 0.0167 train accuracy : 0.9844\n",
            "[epoch:45, iteration:17800] train loss : 0.0138 train accuracy : 1.0000\n",
            "[epoch:45, iteration:17986] test_loss : 0.7027 test accuracy : 0.8688\n",
            "[epoch:46, iteration:18000] train loss : 0.0563 train accuracy : 0.9922\n",
            "[epoch:46, iteration:18200] train loss : 0.0245 train accuracy : 0.9922\n",
            "[epoch:46, iteration:18377] test_loss : 0.7305 test accuracy : 0.8677\n",
            "[epoch:47, iteration:18400] train loss : 0.0241 train accuracy : 0.9844\n",
            "[epoch:47, iteration:18600] train loss : 0.0421 train accuracy : 0.9844\n",
            "[epoch:47, iteration:18768] test_loss : 0.6897 test accuracy : 0.8728\n",
            "[epoch:48, iteration:18800] train loss : 0.0137 train accuracy : 0.9922\n",
            "[epoch:48, iteration:19000] train loss : 0.0109 train accuracy : 0.9922\n",
            "[epoch:48, iteration:19159] test_loss : 0.6885 test accuracy : 0.8738\n",
            "[epoch:49, iteration:19200] train loss : 0.0076 train accuracy : 1.0000\n",
            "[epoch:49, iteration:19400] train loss : 0.0290 train accuracy : 0.9844\n",
            "[epoch:49, iteration:19550] test_loss : 0.6898 test accuracy : 0.8744\n",
            "[epoch:50, iteration:19600] train loss : 0.0267 train accuracy : 0.9844\n",
            "[epoch:50, iteration:19800] train loss : 0.0653 train accuracy : 0.9688\n",
            "[epoch:50, iteration:19941] test_loss : 0.6896 test accuracy : 0.8735\n",
            "[epoch:51, iteration:20000] train loss : 0.0063 train accuracy : 1.0000\n",
            "[epoch:51, iteration:20200] train loss : 0.0022 train accuracy : 1.0000\n",
            "[epoch:51, iteration:20332] test_loss : 0.6731 test accuracy : 0.8748\n",
            "[epoch:52, iteration:20400] train loss : 0.0171 train accuracy : 0.9922\n",
            "[epoch:52, iteration:20600] train loss : 0.0171 train accuracy : 0.9922\n",
            "[epoch:52, iteration:20723] test_loss : 0.7311 test accuracy : 0.8682\n",
            "[epoch:53, iteration:20800] train loss : 0.0257 train accuracy : 0.9922\n",
            "[epoch:53, iteration:21000] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:53, iteration:21114] test_loss : 0.7780 test accuracy : 0.8610\n",
            "[epoch:54, iteration:21200] train loss : 0.0107 train accuracy : 1.0000\n",
            "[epoch:54, iteration:21400] train loss : 0.0064 train accuracy : 1.0000\n",
            "[epoch:54, iteration:21505] test_loss : 0.7432 test accuracy : 0.8712\n",
            "[epoch:55, iteration:21600] train loss : 0.0447 train accuracy : 0.9844\n",
            "[epoch:55, iteration:21800] train loss : 0.0118 train accuracy : 0.9922\n",
            "[epoch:55, iteration:21896] test_loss : 0.7443 test accuracy : 0.8716\n",
            "[epoch:56, iteration:22000] train loss : 0.0087 train accuracy : 0.9922\n",
            "[epoch:56, iteration:22200] train loss : 0.0038 train accuracy : 1.0000\n",
            "[epoch:56, iteration:22287] test_loss : 0.7531 test accuracy : 0.8644\n",
            "[epoch:57, iteration:22400] train loss : 0.0486 train accuracy : 0.9844\n",
            "[epoch:57, iteration:22600] train loss : 0.0094 train accuracy : 1.0000\n",
            "[epoch:57, iteration:22678] test_loss : 0.6924 test accuracy : 0.8711\n",
            "[epoch:58, iteration:22800] train loss : 0.0780 train accuracy : 0.9766\n",
            "[epoch:58, iteration:23000] train loss : 0.0038 train accuracy : 1.0000\n",
            "[epoch:58, iteration:23069] test_loss : 0.7404 test accuracy : 0.8712\n",
            "[epoch:59, iteration:23200] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:59, iteration:23400] train loss : 0.0058 train accuracy : 1.0000\n",
            "[epoch:59, iteration:23460] test_loss : 0.7670 test accuracy : 0.8691\n",
            "[epoch:60, iteration:23600] train loss : 0.0296 train accuracy : 0.9922\n",
            "[epoch:60, iteration:23800] train loss : 0.0120 train accuracy : 0.9922\n",
            "[epoch:60, iteration:23851] test_loss : 0.7265 test accuracy : 0.8735\n",
            "[epoch:61, iteration:24000] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:61, iteration:24200] train loss : 0.0439 train accuracy : 0.9844\n",
            "[epoch:61, iteration:24242] test_loss : 0.7256 test accuracy : 0.8759\n",
            "checkpoint is saved !\n",
            "[epoch:62, iteration:24400] train loss : 0.0007 train accuracy : 1.0000\n",
            "[epoch:62, iteration:24600] train loss : 0.0041 train accuracy : 1.0000\n",
            "[epoch:62, iteration:24633] test_loss : 0.7725 test accuracy : 0.8695\n",
            "[epoch:63, iteration:24800] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:63, iteration:25000] train loss : 0.0082 train accuracy : 1.0000\n",
            "[epoch:63, iteration:25024] test_loss : 0.7619 test accuracy : 0.8684\n",
            "[epoch:64, iteration:25200] train loss : 0.0430 train accuracy : 0.9766\n",
            "[epoch:64, iteration:25400] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:64, iteration:25415] test_loss : 0.7214 test accuracy : 0.8682\n",
            "[epoch:65, iteration:25600] train loss : 0.0121 train accuracy : 0.9922\n",
            "[epoch:65, iteration:25800] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:65, iteration:25806] test_loss : 0.7092 test accuracy : 0.8749\n",
            "[epoch:66, iteration:26000] train loss : 0.0108 train accuracy : 1.0000\n",
            "[epoch:66, iteration:26197] test_loss : 0.7999 test accuracy : 0.8704\n",
            "[epoch:67, iteration:26200] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:67, iteration:26400] train loss : 0.0125 train accuracy : 1.0000\n",
            "[epoch:67, iteration:26588] test_loss : 0.8027 test accuracy : 0.8650\n",
            "[epoch:68, iteration:26600] train loss : 0.0089 train accuracy : 1.0000\n",
            "[epoch:68, iteration:26800] train loss : 0.0110 train accuracy : 1.0000\n",
            "[epoch:68, iteration:26979] test_loss : 0.7380 test accuracy : 0.8777\n",
            "checkpoint is saved !\n",
            "[epoch:69, iteration:27000] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:69, iteration:27200] train loss : 0.0087 train accuracy : 1.0000\n",
            "[epoch:69, iteration:27370] test_loss : 0.7201 test accuracy : 0.8791\n",
            "checkpoint is saved !\n",
            "[epoch:70, iteration:27400] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:70, iteration:27600] train loss : 0.0223 train accuracy : 0.9922\n",
            "[epoch:70, iteration:27761] test_loss : 0.7425 test accuracy : 0.8778\n",
            "[epoch:71, iteration:27800] train loss : 0.0235 train accuracy : 0.9922\n",
            "[epoch:71, iteration:28000] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:71, iteration:28152] test_loss : 0.7335 test accuracy : 0.8767\n",
            "[epoch:72, iteration:28200] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:72, iteration:28400] train loss : 0.0118 train accuracy : 0.9922\n",
            "[epoch:72, iteration:28543] test_loss : 0.8336 test accuracy : 0.8638\n",
            "[epoch:73, iteration:28600] train loss : 0.0061 train accuracy : 1.0000\n",
            "[epoch:73, iteration:28800] train loss : 0.0052 train accuracy : 1.0000\n",
            "[epoch:73, iteration:28934] test_loss : 0.7569 test accuracy : 0.8706\n",
            "[epoch:74, iteration:29000] train loss : 0.0494 train accuracy : 0.9844\n",
            "[epoch:74, iteration:29200] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:74, iteration:29325] test_loss : 0.6974 test accuracy : 0.8779\n",
            "[epoch:75, iteration:29400] train loss : 0.0182 train accuracy : 0.9922\n",
            "[epoch:75, iteration:29600] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:75, iteration:29716] test_loss : 0.7371 test accuracy : 0.8721\n",
            "[epoch:76, iteration:29800] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:76, iteration:30000] train loss : 0.0264 train accuracy : 0.9922\n",
            "[epoch:76, iteration:30107] test_loss : 0.7345 test accuracy : 0.8757\n",
            "[epoch:77, iteration:30200] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:77, iteration:30400] train loss : 0.0036 train accuracy : 1.0000\n",
            "[epoch:77, iteration:30498] test_loss : 0.7617 test accuracy : 0.8668\n",
            "[epoch:78, iteration:30600] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:78, iteration:30800] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:78, iteration:30889] test_loss : 0.7152 test accuracy : 0.8770\n",
            "[epoch:79, iteration:31000] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:79, iteration:31200] train loss : 0.0032 train accuracy : 1.0000\n",
            "[epoch:79, iteration:31280] test_loss : 0.7455 test accuracy : 0.8772\n",
            "[epoch:80, iteration:31400] train loss : 0.0033 train accuracy : 1.0000\n",
            "[epoch:80, iteration:31600] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:80, iteration:31671] test_loss : 0.7029 test accuracy : 0.8823\n",
            "checkpoint is saved !\n",
            "[epoch:81, iteration:31800] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:81, iteration:32000] train loss : 0.0538 train accuracy : 0.9922\n",
            "[epoch:81, iteration:32062] test_loss : 0.8738 test accuracy : 0.8610\n",
            "[epoch:82, iteration:32200] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:82, iteration:32400] train loss : 0.0017 train accuracy : 1.0000\n",
            "[epoch:82, iteration:32453] test_loss : 0.7307 test accuracy : 0.8740\n",
            "[epoch:83, iteration:32600] train loss : 0.0093 train accuracy : 1.0000\n",
            "[epoch:83, iteration:32800] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:83, iteration:32844] test_loss : 0.7248 test accuracy : 0.8785\n",
            "[epoch:84, iteration:33000] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:84, iteration:33200] train loss : 0.0018 train accuracy : 1.0000\n",
            "[epoch:84, iteration:33235] test_loss : 0.7333 test accuracy : 0.8783\n",
            "[epoch:85, iteration:33400] train loss : 0.0429 train accuracy : 0.9922\n",
            "[epoch:85, iteration:33600] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:85, iteration:33626] test_loss : 0.7871 test accuracy : 0.8738\n",
            "[epoch:86, iteration:33800] train loss : 0.0206 train accuracy : 0.9844\n",
            "[epoch:86, iteration:34000] train loss : 0.0371 train accuracy : 0.9844\n",
            "[epoch:86, iteration:34017] test_loss : 0.7680 test accuracy : 0.8763\n",
            "[epoch:87, iteration:34200] train loss : 0.0082 train accuracy : 1.0000\n",
            "[epoch:87, iteration:34400] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:87, iteration:34408] test_loss : 0.7384 test accuracy : 0.8775\n",
            "[epoch:88, iteration:34600] train loss : 0.0027 train accuracy : 1.0000\n",
            "[epoch:88, iteration:34799] test_loss : 0.7704 test accuracy : 0.8787\n",
            "[epoch:89, iteration:34800] train loss : 0.0076 train accuracy : 1.0000\n",
            "[epoch:89, iteration:35000] train loss : 0.0177 train accuracy : 0.9922\n",
            "[epoch:89, iteration:35190] test_loss : 0.8065 test accuracy : 0.8733\n",
            "[epoch:90, iteration:35200] train loss : 0.0128 train accuracy : 0.9922\n",
            "[epoch:90, iteration:35400] train loss : 0.0511 train accuracy : 0.9922\n",
            "[epoch:90, iteration:35581] test_loss : 0.7270 test accuracy : 0.8819\n",
            "[epoch:91, iteration:35600] train loss : 0.0045 train accuracy : 1.0000\n",
            "[epoch:91, iteration:35800] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:91, iteration:35972] test_loss : 0.7561 test accuracy : 0.8760\n",
            "[epoch:92, iteration:36000] train loss : 0.0247 train accuracy : 0.9844\n",
            "[epoch:92, iteration:36200] train loss : 0.0096 train accuracy : 1.0000\n",
            "[epoch:92, iteration:36363] test_loss : 0.7775 test accuracy : 0.8756\n",
            "[epoch:93, iteration:36400] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:93, iteration:36600] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:93, iteration:36754] test_loss : 0.7706 test accuracy : 0.8785\n",
            "[epoch:94, iteration:36800] train loss : 0.0112 train accuracy : 0.9922\n",
            "[epoch:94, iteration:37000] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:94, iteration:37145] test_loss : 0.7640 test accuracy : 0.8771\n",
            "[epoch:95, iteration:37200] train loss : 0.0124 train accuracy : 0.9922\n",
            "[epoch:95, iteration:37400] train loss : 0.0035 train accuracy : 1.0000\n",
            "[epoch:95, iteration:37536] test_loss : 0.7297 test accuracy : 0.8756\n",
            "[epoch:96, iteration:37600] train loss : 0.0106 train accuracy : 0.9922\n",
            "[epoch:96, iteration:37800] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:96, iteration:37927] test_loss : 0.7465 test accuracy : 0.8794\n",
            "[epoch:97, iteration:38000] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:97, iteration:38200] train loss : 0.0124 train accuracy : 0.9922\n",
            "[epoch:97, iteration:38318] test_loss : 0.7805 test accuracy : 0.8771\n",
            "[epoch:98, iteration:38400] train loss : 0.0029 train accuracy : 1.0000\n",
            "[epoch:98, iteration:38600] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:98, iteration:38709] test_loss : 0.7688 test accuracy : 0.8768\n",
            "[epoch:99, iteration:38800] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:99, iteration:39000] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:99, iteration:39100] test_loss : 0.8091 test accuracy : 0.8742\n",
            "[epoch:100, iteration:39200] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:100, iteration:39400] train loss : 0.0131 train accuracy : 0.9922\n",
            "[epoch:100, iteration:39491] test_loss : 0.7658 test accuracy : 0.8771\n",
            "[epoch:101, iteration:39600] train loss : 0.0039 train accuracy : 1.0000\n",
            "[epoch:101, iteration:39800] train loss : 0.0046 train accuracy : 1.0000\n",
            "[epoch:101, iteration:39882] test_loss : 0.7757 test accuracy : 0.8763\n",
            "[epoch:102, iteration:40000] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:102, iteration:40200] train loss : 0.0087 train accuracy : 1.0000\n",
            "[epoch:102, iteration:40273] test_loss : 0.8314 test accuracy : 0.8715\n",
            "[epoch:103, iteration:40400] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:103, iteration:40600] train loss : 0.0064 train accuracy : 1.0000\n",
            "[epoch:103, iteration:40664] test_loss : 0.7686 test accuracy : 0.8789\n",
            "[epoch:104, iteration:40800] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:104, iteration:41000] train loss : 0.0436 train accuracy : 0.9766\n",
            "[epoch:104, iteration:41055] test_loss : 0.8682 test accuracy : 0.8665\n",
            "[epoch:105, iteration:41200] train loss : 0.0140 train accuracy : 0.9922\n",
            "[epoch:105, iteration:41400] train loss : 0.0063 train accuracy : 1.0000\n",
            "[epoch:105, iteration:41446] test_loss : 0.8153 test accuracy : 0.8702\n",
            "[epoch:106, iteration:41600] train loss : 0.0021 train accuracy : 1.0000\n",
            "[epoch:106, iteration:41800] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:106, iteration:41837] test_loss : 0.8037 test accuracy : 0.8750\n",
            "[epoch:107, iteration:42000] train loss : 0.0727 train accuracy : 0.9844\n",
            "[epoch:107, iteration:42200] train loss : 0.0075 train accuracy : 1.0000\n",
            "[epoch:107, iteration:42228] test_loss : 0.7898 test accuracy : 0.8786\n",
            "[epoch:108, iteration:42400] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:108, iteration:42600] train loss : 0.0051 train accuracy : 1.0000\n",
            "[epoch:108, iteration:42619] test_loss : 0.8295 test accuracy : 0.8731\n",
            "[epoch:109, iteration:42800] train loss : 0.0884 train accuracy : 0.9688\n",
            "[epoch:109, iteration:43000] train loss : 0.0219 train accuracy : 0.9922\n",
            "[epoch:109, iteration:43010] test_loss : 0.8162 test accuracy : 0.8715\n",
            "[epoch:110, iteration:43200] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:110, iteration:43400] train loss : 0.0223 train accuracy : 0.9844\n",
            "[epoch:110, iteration:43401] test_loss : 0.8560 test accuracy : 0.8714\n",
            "[epoch:111, iteration:43600] train loss : 0.0062 train accuracy : 0.9922\n",
            "[epoch:111, iteration:43792] test_loss : 0.8192 test accuracy : 0.8751\n",
            "[epoch:112, iteration:43800] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:112, iteration:44000] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:112, iteration:44183] test_loss : 0.7942 test accuracy : 0.8770\n",
            "[epoch:113, iteration:44200] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:113, iteration:44400] train loss : 0.0075 train accuracy : 0.9922\n",
            "[epoch:113, iteration:44574] test_loss : 0.8399 test accuracy : 0.8709\n",
            "[epoch:114, iteration:44600] train loss : 0.0072 train accuracy : 1.0000\n",
            "[epoch:114, iteration:44800] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:114, iteration:44965] test_loss : 0.7860 test accuracy : 0.8767\n",
            "[epoch:115, iteration:45000] train loss : 0.0352 train accuracy : 0.9844\n",
            "[epoch:115, iteration:45200] train loss : 0.0194 train accuracy : 0.9922\n",
            "[epoch:115, iteration:45356] test_loss : 0.8121 test accuracy : 0.8717\n",
            "[epoch:116, iteration:45400] train loss : 0.0066 train accuracy : 1.0000\n",
            "[epoch:116, iteration:45600] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:116, iteration:45747] test_loss : 0.7576 test accuracy : 0.8805\n",
            "[epoch:117, iteration:45800] train loss : 0.0045 train accuracy : 1.0000\n",
            "[epoch:117, iteration:46000] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:117, iteration:46138] test_loss : 0.7735 test accuracy : 0.8774\n",
            "[epoch:118, iteration:46200] train loss : 0.0040 train accuracy : 1.0000\n",
            "[epoch:118, iteration:46400] train loss : 0.0203 train accuracy : 0.9922\n",
            "[epoch:118, iteration:46529] test_loss : 0.7835 test accuracy : 0.8752\n",
            "[epoch:119, iteration:46600] train loss : 0.0039 train accuracy : 1.0000\n",
            "[epoch:119, iteration:46800] train loss : 0.0346 train accuracy : 0.9922\n",
            "[epoch:119, iteration:46920] test_loss : 0.7377 test accuracy : 0.8810\n",
            "[epoch:120, iteration:47000] train loss : 0.0016 train accuracy : 1.0000\n",
            "[epoch:120, iteration:47200] train loss : 0.0017 train accuracy : 1.0000\n",
            "[epoch:120, iteration:47311] test_loss : 0.7765 test accuracy : 0.8785\n",
            "[epoch:121, iteration:47400] train loss : 0.0023 train accuracy : 1.0000\n",
            "[epoch:121, iteration:47600] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:121, iteration:47702] test_loss : 0.8277 test accuracy : 0.8773\n",
            "[epoch:122, iteration:47800] train loss : 0.0271 train accuracy : 0.9844\n",
            "[epoch:122, iteration:48000] train loss : 0.0227 train accuracy : 0.9922\n",
            "[epoch:122, iteration:48093] test_loss : 0.7588 test accuracy : 0.8831\n",
            "checkpoint is saved !\n",
            "[epoch:123, iteration:48200] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:123, iteration:48400] train loss : 0.0067 train accuracy : 0.9922\n",
            "[epoch:123, iteration:48484] test_loss : 0.7899 test accuracy : 0.8764\n",
            "[epoch:124, iteration:48600] train loss : 0.0053 train accuracy : 1.0000\n",
            "[epoch:124, iteration:48800] train loss : 0.0020 train accuracy : 1.0000\n",
            "[epoch:124, iteration:48875] test_loss : 0.7789 test accuracy : 0.8790\n",
            "[epoch:125, iteration:49000] train loss : 0.0130 train accuracy : 0.9922\n",
            "[epoch:125, iteration:49200] train loss : 0.0146 train accuracy : 0.9922\n",
            "[epoch:125, iteration:49266] test_loss : 0.7549 test accuracy : 0.8782\n",
            "[epoch:126, iteration:49400] train loss : 0.0008 train accuracy : 1.0000\n",
            "[epoch:126, iteration:49600] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:126, iteration:49657] test_loss : 0.9003 test accuracy : 0.8670\n",
            "[epoch:127, iteration:49800] train loss : 0.0180 train accuracy : 0.9922\n",
            "[epoch:127, iteration:50000] train loss : 0.0195 train accuracy : 0.9844\n",
            "[epoch:127, iteration:50048] test_loss : 0.7744 test accuracy : 0.8786\n",
            "[epoch:128, iteration:50200] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:128, iteration:50400] train loss : 0.0105 train accuracy : 0.9922\n",
            "[epoch:128, iteration:50439] test_loss : 0.8039 test accuracy : 0.8743\n",
            "[epoch:129, iteration:50600] train loss : 0.0012 train accuracy : 1.0000\n",
            "[epoch:129, iteration:50800] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:129, iteration:50830] test_loss : 0.8257 test accuracy : 0.8767\n",
            "[epoch:130, iteration:51000] train loss : 0.0147 train accuracy : 0.9922\n",
            "[epoch:130, iteration:51200] train loss : 0.0025 train accuracy : 1.0000\n",
            "[epoch:130, iteration:51221] test_loss : 0.8456 test accuracy : 0.8728\n",
            "[epoch:131, iteration:51400] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:131, iteration:51600] train loss : 0.0273 train accuracy : 0.9922\n",
            "[epoch:131, iteration:51612] test_loss : 0.7802 test accuracy : 0.8773\n",
            "[epoch:132, iteration:51800] train loss : 0.0174 train accuracy : 0.9922\n",
            "[epoch:132, iteration:52000] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:132, iteration:52003] test_loss : 0.7837 test accuracy : 0.8790\n",
            "[epoch:133, iteration:52200] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:133, iteration:52394] test_loss : 0.8475 test accuracy : 0.8789\n",
            "[epoch:134, iteration:52400] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:134, iteration:52600] train loss : 0.0153 train accuracy : 1.0000\n",
            "[epoch:134, iteration:52785] test_loss : 0.7835 test accuracy : 0.8804\n",
            "[epoch:135, iteration:52800] train loss : 0.0041 train accuracy : 1.0000\n",
            "[epoch:135, iteration:53000] train loss : 0.0077 train accuracy : 0.9922\n",
            "[epoch:135, iteration:53176] test_loss : 0.8038 test accuracy : 0.8800\n",
            "[epoch:136, iteration:53200] train loss : 0.0032 train accuracy : 1.0000\n",
            "[epoch:136, iteration:53400] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:136, iteration:53567] test_loss : 0.8142 test accuracy : 0.8806\n",
            "[epoch:137, iteration:53600] train loss : 0.0007 train accuracy : 1.0000\n",
            "[epoch:137, iteration:53800] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:137, iteration:53958] test_loss : 0.7993 test accuracy : 0.8826\n",
            "[epoch:138, iteration:54000] train loss : 0.0017 train accuracy : 1.0000\n",
            "[epoch:138, iteration:54200] train loss : 0.0041 train accuracy : 1.0000\n",
            "[epoch:138, iteration:54349] test_loss : 0.8872 test accuracy : 0.8706\n",
            "[epoch:139, iteration:54400] train loss : 0.0293 train accuracy : 0.9844\n",
            "[epoch:139, iteration:54600] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:139, iteration:54740] test_loss : 0.7994 test accuracy : 0.8799\n",
            "[epoch:140, iteration:54800] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:140, iteration:55000] train loss : 0.0525 train accuracy : 0.9922\n",
            "[epoch:140, iteration:55131] test_loss : 0.7934 test accuracy : 0.8804\n",
            "[epoch:141, iteration:55200] train loss : 0.0279 train accuracy : 0.9844\n",
            "[epoch:141, iteration:55400] train loss : 0.0028 train accuracy : 1.0000\n",
            "[epoch:141, iteration:55522] test_loss : 0.8794 test accuracy : 0.8708\n",
            "[epoch:142, iteration:55600] train loss : 0.0544 train accuracy : 0.9922\n",
            "[epoch:142, iteration:55800] train loss : 0.0048 train accuracy : 1.0000\n",
            "[epoch:142, iteration:55913] test_loss : 0.8216 test accuracy : 0.8742\n",
            "[epoch:143, iteration:56000] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:143, iteration:56200] train loss : 0.0016 train accuracy : 1.0000\n",
            "[epoch:143, iteration:56304] test_loss : 0.8051 test accuracy : 0.8796\n",
            "[epoch:144, iteration:56400] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:144, iteration:56600] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:144, iteration:56695] test_loss : 0.7863 test accuracy : 0.8863\n",
            "checkpoint is saved !\n",
            "[epoch:145, iteration:56800] train loss : 0.0014 train accuracy : 1.0000\n",
            "[epoch:145, iteration:57000] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:145, iteration:57086] test_loss : 0.8478 test accuracy : 0.8795\n",
            "[epoch:146, iteration:57200] train loss : 0.0043 train accuracy : 1.0000\n",
            "[epoch:146, iteration:57400] train loss : 0.0166 train accuracy : 0.9922\n",
            "[epoch:146, iteration:57477] test_loss : 0.8282 test accuracy : 0.8745\n",
            "[epoch:147, iteration:57600] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:147, iteration:57800] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:147, iteration:57868] test_loss : 0.7781 test accuracy : 0.8818\n",
            "[epoch:148, iteration:58000] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:148, iteration:58200] train loss : 0.0456 train accuracy : 0.9922\n",
            "[epoch:148, iteration:58259] test_loss : 0.7706 test accuracy : 0.8821\n",
            "[epoch:149, iteration:58400] train loss : 0.0029 train accuracy : 1.0000\n",
            "[epoch:149, iteration:58600] train loss : 0.0134 train accuracy : 0.9922\n",
            "[epoch:149, iteration:58650] test_loss : 0.7683 test accuracy : 0.8836\n",
            "[epoch:150, iteration:58800] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:150, iteration:59000] train loss : 0.0257 train accuracy : 0.9922\n",
            "[epoch:150, iteration:59041] test_loss : 0.7930 test accuracy : 0.8815\n",
            "[epoch:151, iteration:59200] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:151, iteration:59400] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:151, iteration:59432] test_loss : 0.8280 test accuracy : 0.8747\n",
            "[epoch:152, iteration:59600] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:152, iteration:59800] train loss : 0.0031 train accuracy : 1.0000\n",
            "[epoch:152, iteration:59823] test_loss : 0.8797 test accuracy : 0.8713\n",
            "[epoch:153, iteration:60000] train loss : 0.0071 train accuracy : 1.0000\n",
            "[epoch:153, iteration:60200] train loss : 0.0058 train accuracy : 1.0000\n",
            "[epoch:153, iteration:60214] test_loss : 0.7707 test accuracy : 0.8813\n",
            "[epoch:154, iteration:60400] train loss : 0.0039 train accuracy : 1.0000\n",
            "[epoch:154, iteration:60600] train loss : 0.0034 train accuracy : 1.0000\n",
            "[epoch:154, iteration:60605] test_loss : 0.8014 test accuracy : 0.8801\n",
            "[epoch:155, iteration:60800] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:155, iteration:60996] test_loss : 0.8136 test accuracy : 0.8789\n",
            "[epoch:156, iteration:61000] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:156, iteration:61200] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:156, iteration:61387] test_loss : 0.8428 test accuracy : 0.8802\n",
            "[epoch:157, iteration:61400] train loss : 0.0010 train accuracy : 1.0000\n",
            "[epoch:157, iteration:61600] train loss : 0.0013 train accuracy : 1.0000\n",
            "[epoch:157, iteration:61778] test_loss : 0.8743 test accuracy : 0.8746\n",
            "[epoch:158, iteration:61800] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:158, iteration:62000] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:158, iteration:62169] test_loss : 0.7895 test accuracy : 0.8816\n",
            "[epoch:159, iteration:62200] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:159, iteration:62400] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:159, iteration:62560] test_loss : 0.8417 test accuracy : 0.8765\n",
            "[epoch:160, iteration:62600] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:160, iteration:62800] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:160, iteration:62951] test_loss : 0.8503 test accuracy : 0.8751\n",
            "[epoch:161, iteration:63000] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:161, iteration:63200] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:161, iteration:63342] test_loss : 0.7957 test accuracy : 0.8834\n",
            "[epoch:162, iteration:63400] train loss : 0.0194 train accuracy : 0.9844\n",
            "[epoch:162, iteration:63600] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:162, iteration:63733] test_loss : 0.7964 test accuracy : 0.8819\n",
            "[epoch:163, iteration:63800] train loss : 0.0472 train accuracy : 0.9922\n",
            "[epoch:163, iteration:64000] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:163, iteration:64124] test_loss : 0.8062 test accuracy : 0.8851\n",
            "[epoch:164, iteration:64200] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:164, iteration:64400] train loss : 0.0108 train accuracy : 0.9922\n",
            "[epoch:164, iteration:64515] test_loss : 0.8584 test accuracy : 0.8766\n",
            "[epoch:165, iteration:64600] train loss : 0.0517 train accuracy : 0.9922\n",
            "[epoch:165, iteration:64800] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:165, iteration:64906] test_loss : 0.8312 test accuracy : 0.8821\n",
            "[epoch:166, iteration:65000] train loss : 0.0192 train accuracy : 0.9922\n",
            "[epoch:166, iteration:65200] train loss : 0.0081 train accuracy : 0.9922\n",
            "[epoch:166, iteration:65297] test_loss : 0.8357 test accuracy : 0.8783\n",
            "[epoch:167, iteration:65400] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:167, iteration:65600] train loss : 0.0007 train accuracy : 1.0000\n",
            "[epoch:167, iteration:65688] test_loss : 0.8306 test accuracy : 0.8853\n",
            "[epoch:168, iteration:65800] train loss : 0.0000 train accuracy : 1.0000\n",
            "[epoch:168, iteration:66000] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:168, iteration:66079] test_loss : 0.8432 test accuracy : 0.8754\n",
            "[epoch:169, iteration:66200] train loss : 0.0210 train accuracy : 0.9922\n",
            "[epoch:169, iteration:66400] train loss : 0.0039 train accuracy : 1.0000\n",
            "[epoch:169, iteration:66470] test_loss : 0.8344 test accuracy : 0.8798\n",
            "[epoch:170, iteration:66600] train loss : 0.0065 train accuracy : 1.0000\n",
            "[epoch:170, iteration:66800] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:170, iteration:66861] test_loss : 0.8471 test accuracy : 0.8764\n",
            "[epoch:171, iteration:67000] train loss : 0.0009 train accuracy : 1.0000\n",
            "[epoch:171, iteration:67200] train loss : 0.0017 train accuracy : 1.0000\n",
            "[epoch:171, iteration:67252] test_loss : 0.8115 test accuracy : 0.8831\n",
            "[epoch:172, iteration:67400] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:172, iteration:67600] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:172, iteration:67643] test_loss : 0.8462 test accuracy : 0.8791\n",
            "[epoch:173, iteration:67800] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:173, iteration:68000] train loss : 0.0055 train accuracy : 1.0000\n",
            "[epoch:173, iteration:68034] test_loss : 0.7928 test accuracy : 0.8855\n",
            "[epoch:174, iteration:68200] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:174, iteration:68400] train loss : 0.0064 train accuracy : 0.9922\n",
            "[epoch:174, iteration:68425] test_loss : 0.8083 test accuracy : 0.8821\n",
            "[epoch:175, iteration:68600] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:175, iteration:68800] train loss : 0.0127 train accuracy : 0.9922\n",
            "[epoch:175, iteration:68816] test_loss : 0.8945 test accuracy : 0.8779\n",
            "[epoch:176, iteration:69000] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:176, iteration:69200] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:176, iteration:69207] test_loss : 0.8469 test accuracy : 0.8748\n",
            "[epoch:177, iteration:69400] train loss : 0.0005 train accuracy : 1.0000\n",
            "[epoch:177, iteration:69598] test_loss : 0.8512 test accuracy : 0.8805\n",
            "[epoch:178, iteration:69600] train loss : 0.0044 train accuracy : 1.0000\n",
            "[epoch:178, iteration:69800] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:178, iteration:69989] test_loss : 0.8409 test accuracy : 0.8825\n",
            "[epoch:179, iteration:70000] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:179, iteration:70200] train loss : 0.0002 train accuracy : 1.0000\n",
            "[epoch:179, iteration:70380] test_loss : 0.8851 test accuracy : 0.8723\n",
            "[epoch:180, iteration:70400] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:180, iteration:70600] train loss : 0.0011 train accuracy : 1.0000\n",
            "[epoch:180, iteration:70771] test_loss : 0.9418 test accuracy : 0.8683\n",
            "[epoch:181, iteration:70800] train loss : 0.0052 train accuracy : 1.0000\n",
            "[epoch:181, iteration:71000] train loss : 0.0347 train accuracy : 0.9922\n",
            "[epoch:181, iteration:71162] test_loss : 0.8096 test accuracy : 0.8804\n",
            "[epoch:182, iteration:71200] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:182, iteration:71400] train loss : 0.0030 train accuracy : 1.0000\n",
            "[epoch:182, iteration:71553] test_loss : 0.7547 test accuracy : 0.8891\n",
            "checkpoint is saved !\n",
            "[epoch:183, iteration:71600] train loss : 0.0000 train accuracy : 1.0000\n",
            "[epoch:183, iteration:71800] train loss : 0.0001 train accuracy : 1.0000\n",
            "[epoch:183, iteration:71944] test_loss : 0.8200 test accuracy : 0.8825\n",
            "[epoch:184, iteration:72000] train loss : 0.0000 train accuracy : 1.0000\n",
            "[epoch:184, iteration:72200] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:184, iteration:72335] test_loss : 0.8778 test accuracy : 0.8760\n",
            "[epoch:185, iteration:72400] train loss : 0.0062 train accuracy : 1.0000\n",
            "[epoch:185, iteration:72600] train loss : 0.0467 train accuracy : 0.9922\n",
            "[epoch:185, iteration:72726] test_loss : 0.8414 test accuracy : 0.8817\n",
            "[epoch:186, iteration:72800] train loss : 0.0220 train accuracy : 0.9922\n",
            "[epoch:186, iteration:73000] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:186, iteration:73117] test_loss : 0.8369 test accuracy : 0.8809\n",
            "[epoch:187, iteration:73200] train loss : 0.0333 train accuracy : 0.9844\n",
            "[epoch:187, iteration:73400] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:187, iteration:73508] test_loss : 0.7732 test accuracy : 0.8871\n",
            "[epoch:188, iteration:73600] train loss : 0.0232 train accuracy : 0.9922\n",
            "[epoch:188, iteration:73800] train loss : 0.0026 train accuracy : 1.0000\n",
            "[epoch:188, iteration:73899] test_loss : 0.8440 test accuracy : 0.8772\n",
            "[epoch:189, iteration:74000] train loss : 0.0023 train accuracy : 1.0000\n",
            "[epoch:189, iteration:74200] train loss : 0.0006 train accuracy : 1.0000\n",
            "[epoch:189, iteration:74290] test_loss : 0.8253 test accuracy : 0.8832\n",
            "[epoch:190, iteration:74400] train loss : 0.0160 train accuracy : 0.9922\n",
            "[epoch:190, iteration:74600] train loss : 0.0027 train accuracy : 1.0000\n",
            "[epoch:190, iteration:74681] test_loss : 0.8031 test accuracy : 0.8855\n",
            "[epoch:191, iteration:74800] train loss : 0.0062 train accuracy : 0.9922\n",
            "[epoch:191, iteration:75000] train loss : 0.0019 train accuracy : 1.0000\n",
            "[epoch:191, iteration:75072] test_loss : 0.8191 test accuracy : 0.8848\n",
            "[epoch:192, iteration:75200] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:192, iteration:75400] train loss : 0.0042 train accuracy : 1.0000\n",
            "[epoch:192, iteration:75463] test_loss : 0.8975 test accuracy : 0.8754\n",
            "[epoch:193, iteration:75600] train loss : 0.0024 train accuracy : 1.0000\n",
            "[epoch:193, iteration:75800] train loss : 0.0054 train accuracy : 1.0000\n",
            "[epoch:193, iteration:75854] test_loss : 0.8309 test accuracy : 0.8780\n",
            "[epoch:194, iteration:76000] train loss : 0.0207 train accuracy : 0.9922\n",
            "[epoch:194, iteration:76200] train loss : 0.0077 train accuracy : 1.0000\n",
            "[epoch:194, iteration:76245] test_loss : 0.8208 test accuracy : 0.8801\n",
            "[epoch:195, iteration:76400] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:195, iteration:76600] train loss : 0.0035 train accuracy : 1.0000\n",
            "[epoch:195, iteration:76636] test_loss : 0.8162 test accuracy : 0.8847\n",
            "[epoch:196, iteration:76800] train loss : 0.0005 train accuracy : 1.0000\n",
            "[epoch:196, iteration:77000] train loss : 0.0066 train accuracy : 0.9922\n",
            "[epoch:196, iteration:77027] test_loss : 0.7847 test accuracy : 0.8898\n",
            "checkpoint is saved !\n",
            "[epoch:197, iteration:77200] train loss : 0.0016 train accuracy : 1.0000\n",
            "[epoch:197, iteration:77400] train loss : 0.0015 train accuracy : 1.0000\n",
            "[epoch:197, iteration:77418] test_loss : 0.8354 test accuracy : 0.8825\n",
            "[epoch:198, iteration:77600] train loss : 0.0000 train accuracy : 1.0000\n",
            "[epoch:198, iteration:77800] train loss : 0.0004 train accuracy : 1.0000\n",
            "[epoch:198, iteration:77809] test_loss : 0.8370 test accuracy : 0.8819\n",
            "[epoch:199, iteration:78000] train loss : 0.0003 train accuracy : 1.0000\n",
            "[epoch:199, iteration:78200] train loss : 0.0242 train accuracy : 0.9875\n",
            "[epoch:199, iteration:78200] test_loss : 0.8588 test accuracy : 0.8825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECu3yS0OvfoR",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Visualize and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sqVp-vLRy",
        "colab_type": "code",
        "outputId": "80e4006e-4200-4f0b-ed62-df7475ab900b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "\n",
        "if not training_process:\n",
        "  # Re-load trained model\n",
        "  my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "  optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "  # Testing\n",
        "  n = 0.\n",
        "  test_loss = 0.\n",
        "  test_acc = 0.\n",
        "  my_classifier.eval()\n",
        "  for test_inputs, test_labels in test_dataloader:\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    logits = my_classifier(test_inputs)\n",
        "    test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "    test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "    n += test_inputs.size(0)\n",
        "\n",
        "  test_loss /= n\n",
        "  test_acc /= n\n",
        "  print('Test_loss : {:.4f}, Test accuracy : {:.4f}'.format(test_loss, test_acc))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3p3ullX83wkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjD9SGrvhzc",
        "colab_type": "code",
        "outputId": "e50a9ffa-2b1d-4383-bfce-5f4a88a89bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "my_classifier.eval()\n",
        "\n",
        "num_test_samples = len(test_dataset)\n",
        "random_idx = random.randint(0, num_test_samples)\n",
        "\n",
        "test_input, test_label = test_dataset.__getitem__(random_idx)\n",
        "test_prediction = F.softmax(my_classifier(test_input.unsqueeze(0).to(device)), dim=1).argmax().item()\n",
        "print('label : %s' % classes[test_label])\n",
        "print('prediction : %s' % classes[test_prediction])\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(test_input))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label : cat\n",
            "prediction : cat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwcY8z_C2QLA",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2W0ZtF82Xyb",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\"> Fill here with your discussion </font>"
      ]
    }
  ]
}